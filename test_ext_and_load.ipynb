{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "import fire\n",
    "\n",
    "from llama import Dialog, Llama\n",
    "import torch.distributed as dist\n",
    "import torch\n",
    "import os\n",
    "from data_casual import output_list_train, input_list_train, input_list_test, output_list_test\n",
    "from eval import extracting_steering_vector, calc_loss_steering_vector\n",
    "\n",
    "ckpt_dir = \"./\"\n",
    "tokenizer_path = \"./tokenizer.model\"\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\llama\\generation.py:98: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
      "c:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\torch\\__init__.py:1144: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\tensor\\python_tensor.cpp:434.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in 151.64 seconds\n",
      "<llama.generation.Llama object at 0x000002873BFE9350>\n"
     ]
    }
   ],
   "source": [
    "generator = Llama.build(ckpt_dir=ckpt_dir, tokenizer_path=tokenizer_path, max_seq_len= 1024, max_batch_size= 4, activation=True, activation_layer=12)\n",
    "print(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.6\n",
    "top_p = 0.9\n",
    "max_seq_len = 1024\n",
    "max_batch_size = 4\n",
    "max_gen_len= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'Answer with T if you believe the following statement is correct, answer with F if you believe it is false, do not give any reasoning.'},\n",
       " {'role': 'system',\n",
       "  'content': 'You are a helpful assistant in casual reasoning, is the following reasoning true?'},\n",
       " {'role': 'user',\n",
       "  'content': 'Premise: Suppose there is a closed system of 4 variables, A, B, C and D. All the statistical relations among these 4 variables are as follows: A correlates with C. A correlates with D. B correlates with C. B correlates with D. C correlates with D. However, A is independent of B. A and D are independent given B and C. A and D are independent given C. B and D are independent given A and C. B and D are independent given C.\\nHypothesis: A directly causes C.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_list_train[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26221170395869187 0.1549053356282272\n"
     ]
    }
   ],
   "source": [
    "#checking the random baseline\n",
    "average_loss = 0\n",
    "for k in range(1000):\n",
    "    p = output_list_test.count(\"1\")/len(input_list_test)\n",
    "    x = stats.bernoulli.rvs(p, size = len(input_list_test))\n",
    "    loss = 1/len(input_list_test)*sum([abs(int(x[i])-int(k)) for i, k in enumerate(output_list_test)])\n",
    "    average_loss += 1/1000*loss\n",
    "print(average_loss, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\eval\\eval.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  instruct_vec = torch.load(f\"./steering_vectors/instruct_vector{layer}.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss is 0.006396609213839021\n",
      "Training loss is 0.006454936957430255\n",
      "Training loss is 0.006377166632641943\n",
      "Training loss is 0.006352863406145596\n",
      "Training loss is 0.0065132647010214895\n",
      "Training loss is 0.006425773085634638\n",
      "Training loss is 0.006459797602729525\n",
      "Training loss is 0.00631397824375144\n",
      "Training loss is 0.006348002760846326\n",
      "Training loss is 0.006425773085634638\n",
      "Training loss is 0.006377166632641943\n",
      "Training loss is 0.006299396307853631\n",
      "Training loss is 0.006377166632641943\n",
      "Training loss is 0.0062653717907587446\n",
      "Training loss is 0.006348002760846326\n",
      "Training loss is 0.006440355021532447\n",
      "Training loss is 0.006382027277941213\n",
      "Training loss is 0.006435494376233177\n",
      "Training loss is 0.006401469859138291\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m average_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m32\u001b[39m):\n\u001b[1;32m----> 4\u001b[0m     steering_vector, loss \u001b[38;5;241m=\u001b[39m \u001b[43mextracting_steering_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_list_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_list_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     steering_vec \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mnormalize(steering_vector, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      6\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(steering_vector, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./vectors_last_token/vector\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\eval\\eval.py:23\u001b[0m, in \u001b[0;36mextracting_steering_vector\u001b[1;34m(generator, data, layer, iter)\u001b[0m\n\u001b[0;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28miter\u001b[39m):\n\u001b[1;32m---> 23\u001b[0m     value, activation_vec_list \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_list_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mmax_gen_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_gen_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     value_numerical \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m value[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value_numerical \u001b[38;5;241m==\u001b[39m output_list_train[i]:\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\llama\\generation.py:325\u001b[0m, in \u001b[0;36mLlama.chat_completion\u001b[1;34m(self, dialogs, temperature, top_p, max_gen_len, logprobs)\u001b[0m\n\u001b[0;32m    320\u001b[0m     max_gen_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mmax_seq_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    322\u001b[0m prompt_tokens \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformatter\u001b[38;5;241m.\u001b[39mencode_dialog_prompt(dialog) \u001b[38;5;28;01mfor\u001b[39;00m dialog \u001b[38;5;129;01min\u001b[39;00m dialogs\n\u001b[0;32m    324\u001b[0m ]\n\u001b[1;32m--> 325\u001b[0m generation_tokens, generation_logprobs, activations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_gen_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_gen_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logprobs:\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    334\u001b[0m         {\n\u001b[0;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m t, logprobs_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(generation_tokens, generation_logprobs)\n\u001b[0;32m    343\u001b[0m     ], activations\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\llama\\generation.py:181\u001b[0m, in \u001b[0;36mLlama.generate\u001b[1;34m(self, prompt_tokens, max_gen_len, temperature, top_p, logprobs, echo)\u001b[0m\n\u001b[0;32m    179\u001b[0m stop_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mstop_tokens))\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cur_pos \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(min_prompt_len, total_len):\n\u001b[1;32m--> 181\u001b[0m     logits, activation_vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_pos\u001b[49m\u001b[43m:\u001b[49m\u001b[43mcur_pos\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_pos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m temperature \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    183\u001b[0m         probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m temperature, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\llama\\model.py:310\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, tokens, start_pos)\u001b[0m\n\u001b[0;32m    308\u001b[0m activation_vector \u001b[38;5;241m=\u001b[39m activation_tensor[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:]\n\u001b[0;32m    309\u001b[0m \u001b[38;5;66;03m#activation_vector = activation_tensor.mean(dim=0)\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mequal(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madding_activation_vector, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcuda()):\n\u001b[0;32m    311\u001b[0m     steering_vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madding_activation_vector\u001b[38;5;241m.\u001b[39mrepeat(h\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    312\u001b[0m     steering_vector \u001b[38;5;241m=\u001b[39m steering_vector\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#extracting a steering vector for every layer\n",
    "average_loss = 0\n",
    "for k in range(10,32):\n",
    "    steering_vector, loss = extracting_steering_vector(generator, (input_list_train, output_list_train),layer=k, iter=2000)\n",
    "    steering_vec = torch.nn.functional.normalize(steering_vector, dim=0)\n",
    "    torch.save(steering_vector, f\"./vectors_last_token/vector{k}.pt\")\n",
    "    print(f\"Training loss is {loss}\")\n",
    "    average_loss += 1/len(generator.model.layers)*loss\n",
    "print(f\"Average loss is {average_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ribbe\\AppData\\Local\\Temp\\ipykernel_22676\\270925784.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  st_vec = torch.load(f\"./vectors_last_token/vector{k}.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.9219) tensor(-0.0004)\n",
      "tensor(2.9844) tensor(-0.0003)\n",
      "tensor(3.1562) tensor(6.7711e-05)\n",
      "tensor(3.1875) tensor(-6.7711e-05)\n",
      "tensor(3.3125) tensor(-0.0002)\n",
      "tensor(3.2812) tensor(-3.6955e-05)\n",
      "tensor(3.3281) tensor(-0.0008)\n",
      "tensor(3.5781) tensor(-0.0006)\n",
      "tensor(3.7344) tensor(-0.0009)\n",
      "tensor(3.7656) tensor(-0.0005)\n",
      "tensor(4.6250) tensor(0.0005)\n",
      "tensor(4.1875) tensor(-0.0006)\n",
      "tensor(4.6250) tensor(-0.0002)\n",
      "tensor(5.5938) tensor(-0.0010)\n",
      "tensor(6.0312) tensor(-0.0014)\n",
      "tensor(6.7500) tensor(-0.0016)\n",
      "tensor(7.4062) tensor(-0.0003)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './vectors_last_token/vector29.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#analysing the steering vectors\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m32\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     st_vec \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./vectors_last_token/vector\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mk\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mnorm(st_vec, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), torch\u001b[38;5;241m.\u001b[39mmean(st_vec, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)) \n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\torch\\serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1317\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1321\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1322\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1323\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\torch\\serialization.py:659\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    661\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\torch\\serialization.py:640\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 640\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './vectors_last_token/vector29.pt'"
     ]
    }
   ],
   "source": [
    "#analysing the steering vectors\n",
    "for k in range(12, 32):\n",
    "    st_vec = torch.load(f\"./vectors_last_token/vector{k}.pt\")\n",
    "    print(torch.norm(st_vec, dim=0), torch.mean(st_vec, dim=0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loss is 0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ribbe\\AppData\\Local\\Temp\\ipykernel_15724\\2526441504.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  steering_vec = torch.load(f\"./vectors_last_token/vector{k}.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 questions were wrongly classified\n",
      "Layer 12 loss with multiplier 1 is 0.6712564543889873\n",
      "7 questions were wrongly classified\n",
      "Layer 12 loss with multiplier 3 is 0.36574870912219953\n",
      "1162 questions were wrongly classified\n",
      "Layer 12 loss with multiplier 5 is 1.0000000000000189\n",
      "0 questions were wrongly classified\n",
      "Layer 13 loss with multiplier 1 is 0.4836488812392373\n",
      "0 questions were wrongly classified\n",
      "Layer 13 loss with multiplier 3 is 0.15490533562822684\n",
      "1162 questions were wrongly classified\n",
      "Layer 13 loss with multiplier 5 is 1.0000000000000189\n",
      "0 questions were wrongly classified\n",
      "Layer 14 loss with multiplier 1 is 0.5154905335628178\n",
      "0 questions were wrongly classified\n",
      "Layer 14 loss with multiplier 3 is 0.2641996557659188\n",
      "1162 questions were wrongly classified\n",
      "Layer 14 loss with multiplier 5 is 1.0000000000000189\n",
      "0 questions were wrongly classified\n",
      "Layer 15 loss with multiplier 1 is 0.7220309810671309\n",
      "0 questions were wrongly classified\n",
      "Layer 15 loss with multiplier 3 is 0.28657487091221795\n",
      "1162 questions were wrongly classified\n",
      "Layer 15 loss with multiplier 5 is 1.0000000000000189\n",
      "0 questions were wrongly classified\n",
      "Layer 16 loss with multiplier 1 is 0.7702237521514707\n",
      "0 questions were wrongly classified\n",
      "Layer 16 loss with multiplier 3 is 0.8442340791738495\n",
      "0 questions were wrongly classified\n",
      "Layer 16 loss with multiplier 5 is 0.8450946643717842\n",
      "1162 questions were wrongly classified\n",
      "Layer 16 loss with multiplier 7 is 1.0000000000000189\n",
      "0 questions were wrongly classified\n",
      "Layer 17 loss with multiplier 1 is 0.6308089500860593\n",
      "0 questions were wrongly classified\n",
      "Layer 17 loss with multiplier 3 is 0.8175559380378757\n",
      "0 questions were wrongly classified\n",
      "Layer 17 loss with multiplier 5 is 0.8450946643717842\n",
      "329 questions were wrongly classified\n",
      "Layer 17 loss with multiplier 7 is 0.8872633390705814\n"
     ]
    }
   ],
   "source": [
    "#calculating the loss for each layes\n",
    "#initial_loss = calc_loss_steering_vector(generator, torch.tensor(4096*[0]), (input_list_test, output_list_test), layer=12, iter=len(input_list_test), multiplier=0)\n",
    "losses_last_token_layer = {}\n",
    "print(f\"initial loss is {0.68}\")\n",
    "for k in range(12,32):\n",
    "    steering_vec = torch.load(f\"./vectors_last_token/vector{k}.pt\")\n",
    "    for multiplier in range(1,25,2):\n",
    "        loss = calc_loss_steering_vector(generator, steering_vec, (input_list_test, output_list_test), layer=k, iter=len(input_list_test), multiplier=multiplier)\n",
    "        print(f\"Layer {k} loss with multiplier {multiplier} is {loss}\")\n",
    "        losses_last_token_layer[f\"{k},{multiplier}\"] = loss\n",
    "        if loss>0.85:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loss is 0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ribbe\\AppData\\Local\\Temp\\ipykernel_22676\\1496298509.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  steering_vec = torch.load(f\"./vectors_last_token/vector{k}.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 questions were wrongly classified\n",
      "Layer 18 loss with multiplier 1 is 0.6419965576592096\n",
      "0 questions were wrongly classified\n",
      "Layer 18 loss with multiplier 3 is 0.8055077452667908\n",
      "0 questions were wrongly classified\n",
      "Layer 18 loss with multiplier 5 is 0.8433734939759149\n",
      "0 questions were wrongly classified\n",
      "Layer 18 loss with multiplier 7 is 0.7426850258175622\n",
      "1162 questions were wrongly classified\n",
      "Layer 18 loss with multiplier 9 is 1.0000000000000189\n",
      "0 questions were wrongly classified\n",
      "Layer 19 loss with multiplier 1 is 0.6876075731497454\n",
      "0 questions were wrongly classified\n",
      "Layer 19 loss with multiplier 3 is 0.7960413080895098\n",
      "0 questions were wrongly classified\n",
      "Layer 19 loss with multiplier 5 is 0.8442340791738495\n",
      "0 questions were wrongly classified\n",
      "Layer 19 loss with multiplier 7 is 0.8450946643717842\n",
      "390 questions were wrongly classified\n",
      "Layer 19 loss with multiplier 9 is 0.8958691910499278\n",
      "0 questions were wrongly classified\n",
      "Layer 20 loss with multiplier 1 is 0.7177280550774577\n",
      "0 questions were wrongly classified\n",
      "Layer 20 loss with multiplier 3 is 0.8450946643717842\n",
      "0 questions were wrongly classified\n",
      "Layer 20 loss with multiplier 5 is 0.8450946643717842\n",
      "334 questions were wrongly classified\n",
      "Layer 20 loss with multiplier 7 is 0.8958691910499278\n",
      "0 questions were wrongly classified\n",
      "Layer 21 loss with multiplier 1 is 0.6858864027538761\n",
      "0 questions were wrongly classified\n",
      "Layer 21 loss with multiplier 3 is 0.7659208261617975\n",
      "0 questions were wrongly classified\n",
      "Layer 21 loss with multiplier 5 is 0.8339070567986339\n",
      "650 questions were wrongly classified\n",
      "Layer 21 loss with multiplier 7 is 0.8261617900172221\n",
      "1156 questions were wrongly classified\n",
      "Layer 21 loss with multiplier 9 is 0.9948364888124112\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m steering_vec \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./vectors_last_token/vector\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m multiplier \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m25\u001b[39m,\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m----> 8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_loss_steering_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteering_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_list_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_list_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_list_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiplier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultiplier\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m loss with multiplier \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmultiplier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m     losses_last_token_layer[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmultiplier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\eval\\eval.py:56\u001b[0m, in \u001b[0;36mcalc_loss_steering_vector\u001b[1;34m(generator, steering_vec, data, layer, iter, multiplier)\u001b[0m\n\u001b[0;32m     54\u001b[0m wrong_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28miter\u001b[39m):\n\u001b[1;32m---> 56\u001b[0m     value, activations \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_list_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mmax_gen_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_gen_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     61\u001b[0m         value2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\llama\\generation.py:325\u001b[0m, in \u001b[0;36mLlama.chat_completion\u001b[1;34m(self, dialogs, temperature, top_p, max_gen_len, logprobs)\u001b[0m\n\u001b[0;32m    320\u001b[0m     max_gen_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mmax_seq_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    322\u001b[0m prompt_tokens \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformatter\u001b[38;5;241m.\u001b[39mencode_dialog_prompt(dialog) \u001b[38;5;28;01mfor\u001b[39;00m dialog \u001b[38;5;129;01min\u001b[39;00m dialogs\n\u001b[0;32m    324\u001b[0m ]\n\u001b[1;32m--> 325\u001b[0m generation_tokens, generation_logprobs, activations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_gen_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_gen_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logprobs:\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    334\u001b[0m         {\n\u001b[0;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m t, logprobs_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(generation_tokens, generation_logprobs)\n\u001b[0;32m    343\u001b[0m     ], activations\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\llama\\generation.py:181\u001b[0m, in \u001b[0;36mLlama.generate\u001b[1;34m(self, prompt_tokens, max_gen_len, temperature, top_p, logprobs, echo)\u001b[0m\n\u001b[0;32m    179\u001b[0m stop_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mstop_tokens))\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cur_pos \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(min_prompt_len, total_len):\n\u001b[1;32m--> 181\u001b[0m     logits, activation_vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_pos\u001b[49m\u001b[43m:\u001b[49m\u001b[43mcur_pos\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_pos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m temperature \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    183\u001b[0m         probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m temperature, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\llama\\model.py:304\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, tokens, start_pos)\u001b[0m\n\u001b[0;32m    299\u001b[0m     mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhstack(\n\u001b[0;32m    300\u001b[0m         [torch\u001b[38;5;241m.\u001b[39mzeros((seqlen, start_pos), device\u001b[38;5;241m=\u001b[39mtokens\u001b[38;5;241m.\u001b[39mdevice), mask]\n\u001b[0;32m    301\u001b[0m     )\u001b[38;5;241m.\u001b[39mtype_as(h)\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[1;32m--> 304\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_layer_n \u001b[38;5;241m==\u001b[39m k:\n\u001b[0;32m    306\u001b[0m         activation_tensor \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mclone()\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\llama\\model.py:246\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x, start_pos, freqs_cis, mask)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    241\u001b[0m     x: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    244\u001b[0m     mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[0;32m    245\u001b[0m ):\n\u001b[1;32m--> 246\u001b[0m     h \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, start_pos, freqs_cis, mask)\n\u001b[0;32m    247\u001b[0m     out \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn_norm(h))\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\llama\\model.py:45\u001b[0m, in \u001b[0;36mRMSNorm.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 45\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_norm(\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mtype_as(x)\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#calculating the loss for each layes\n",
    "#initial_loss = calc_loss_steering_vector(generator, torch.tensor(4096*[0]), (input_list_test, output_list_test), layer=12, iter=len(input_list_test), multiplier=0)\n",
    "losses_last_token_layer = {}\n",
    "print(f\"initial loss is {0.68}\")\n",
    "for k in range(18,32):\n",
    "    steering_vec = torch.load(f\"./vectors_last_token/vector{k}.pt\")\n",
    "    for multiplier in range(1,25,2):\n",
    "        loss = calc_loss_steering_vector(generator, steering_vec, (input_list_test, output_list_test), layer=k, iter=len(input_list_test), multiplier=multiplier)\n",
    "        print(f\"Layer {k} loss with multiplier {multiplier} is {loss}\")\n",
    "        losses_last_token_layer[f\"{k},{multiplier}\"] = loss\n",
    "        if loss>0.85:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ribbe\\AppData\\Local\\Temp\\ipykernel_15880\\4185949204.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vec = torch.load(f\"./steering_without_instruct/vector{k}.pt\")\n"
     ]
    }
   ],
   "source": [
    "for k in range(10,32):\n",
    "    vec = torch.load(f\"./steering_without_instruct/vector{k}.pt\")\n",
    "    vec = torch.nn.functional.normalize(vec, dim=0)\n",
    "    torch.save(vec, f\"./steering_without_instruct/n_vector{k}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1162"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses2 = {}\n",
    "initial_loss = 0.68\n",
    "len(input_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ribbe\\AppData\\Local\\Temp\\ipykernel_3580\\158326343.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  v = torch.load(f\"./steering_without_instruct/vector{k}.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 8, 8, 8, 9, 10, 10, 11, 12, 18]\n"
     ]
    }
   ],
   "source": [
    "norms = []\n",
    "for k in range(32):\n",
    "    v = torch.load(f\"./steering_without_instruct/vector{k}.pt\")\n",
    "    norm = torch.norm(v)\n",
    "    norm = math.ceil(norm.item())\n",
    "    norms.append(norm)\n",
    "\n",
    "print(norms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loss is 0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ribbe\\AppData\\Local\\Temp\\ipykernel_3580\\2461276629.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  steering_vec = torch.load(f\"./c_and_n_vectors/vector{k}.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 16 loss with multiplier 1 is 0.660068846815837\n",
      "Layer 16 loss with multiplier 3 is 0.5955249569707391\n",
      "Layer 16 loss with multiplier 5 is 0.5481927710843341\n",
      "Layer 16 loss with multiplier 7 is 0.44836488812391945\n",
      "Layer 16 loss with multiplier 9 is 0.28227194492254504\n",
      "Layer 16 loss with multiplier 11 is 0.9991394148020843\n",
      "Layer 17 loss with multiplier 1 is 0.7125645438898499\n",
      "Layer 17 loss with multiplier 3 is 0.809810671256464\n",
      "Layer 17 loss with multiplier 5 is 0.8390705679862417\n",
      "Layer 17 loss with multiplier 7 is 0.8450946643717842\n",
      "Layer 17 loss with multiplier 9 is 0.8433734939759149\n",
      "Layer 17 loss with multiplier 11 is 0.8950086058519932\n",
      "Layer 17 loss with multiplier 13 is 0.9991394148020843\n",
      "Layer 18 loss with multiplier 1 is 0.7512908777969086\n",
      "Layer 18 loss with multiplier 3 is 0.8261617900172221\n",
      "Layer 18 loss with multiplier 5 is 0.838209982788307\n",
      "Layer 18 loss with multiplier 7 is 0.8347676419965685\n",
      "Layer 18 loss with multiplier 9 is 0.8046471600688562\n",
      "Layer 18 loss with multiplier 11 is 0.6841652323580069\n",
      "Layer 18 loss with multiplier 13 is 0.9931153184165419\n",
      "Layer 19 loss with multiplier 1 is 0.6592082616179024\n",
      "Layer 19 loss with multiplier 3 is 0.700516351118765\n",
      "Layer 19 loss with multiplier 5 is 0.7607573149741896\n",
      "Layer 19 loss with multiplier 7 is 0.709982788296046\n",
      "Layer 19 loss with multiplier 9 is 0.4991394148020598\n",
      "Layer 19 loss with multiplier 11 is 0.3141135972461246\n",
      "Layer 19 loss with multiplier 13 is 0.9819277108433916\n",
      "Layer 20 loss with multiplier 1 is 0.6230636833046476\n",
      "Layer 20 loss with multiplier 3 is 0.6901893287435493\n",
      "Layer 20 loss with multiplier 5 is 0.7340791738382159\n",
      "Layer 20 loss with multiplier 7 is 0.679001721170399\n",
      "Layer 20 loss with multiplier 9 is 0.46299483648880735\n",
      "Layer 20 loss with multiplier 11 is 0.26678141135972255\n",
      "Layer 20 loss with multiplier 13 is 0.8580034423408037\n",
      "Layer 20 loss with multiplier 15 is 1.0000000000000189\n",
      "Layer 21 loss with multiplier 1 is 0.6531841652323599\n",
      "Layer 21 loss with multiplier 3 is 0.6807228915662683\n",
      "Layer 21 loss with multiplier 5 is 0.6333907056798632\n",
      "Layer 21 loss with multiplier 7 is 0.5111876075731446\n",
      "Layer 21 loss with multiplier 9 is 0.2848537005163488\n",
      "Layer 21 loss with multiplier 11 is 0.21686746987951677\n",
      "Layer 21 loss with multiplier 13 is 0.6643717728055102\n",
      "Layer 21 loss with multiplier 15 is 0.9870912220309994\n",
      "Layer 22 loss with multiplier 1 is 0.7030981067125689\n",
      "Layer 22 loss with multiplier 3 is 0.7762478485370131\n",
      "Layer 22 loss with multiplier 5 is 0.809810671256464\n",
      "Layer 22 loss with multiplier 7 is 0.8012048192771176\n",
      "Layer 22 loss with multiplier 9 is 0.7263339070568041\n",
      "Layer 22 loss with multiplier 11 is 0.6316695352839939\n",
      "Layer 22 loss with multiplier 13 is 0.8614457831325423\n",
      "Layer 22 loss with multiplier 15 is 0.9965576592082804\n",
      "Layer 23 loss with multiplier 1 is 0.7203098106712617\n",
      "Layer 23 loss with multiplier 3 is 0.819277108433745\n",
      "Layer 23 loss with multiplier 5 is 0.8227194492254836\n",
      "Layer 23 loss with multiplier 7 is 0.7762478485370131\n",
      "Layer 23 loss with multiplier 9 is 0.6583476764199677\n",
      "Layer 23 loss with multiplier 11 is 0.4302925989672932\n",
      "Layer 23 loss with multiplier 13 is 0.5516351118760726\n",
      "Layer 23 loss with multiplier 15 is 0.9440619621342675\n",
      "Layer 24 loss with multiplier 1 is 0.6402753872633403\n",
      "Layer 24 loss with multiplier 3 is 0.5903614457831313\n",
      "Layer 24 loss with multiplier 5 is 0.49741824440619065\n",
      "Layer 24 loss with multiplier 7 is 0.3390705679862275\n",
      "Layer 24 loss with multiplier 9 is 0.2254733218588626\n",
      "Layer 24 loss with multiplier 11 is 0.19449225473321763\n",
      "Layer 24 loss with multiplier 13 is 0.5464716006884648\n",
      "Layer 24 loss with multiplier 15 is 0.9475043029260061\n",
      "Layer 25 loss with multiplier 1 is 0.6540447504302945\n",
      "Layer 25 loss with multiplier 3 is 0.6024096385542163\n",
      "Layer 25 loss with multiplier 5 is 0.4888123924268448\n",
      "Layer 25 loss with multiplier 7 is 0.41308089500860157\n",
      "Layer 25 loss with multiplier 9 is 0.32013769363166666\n",
      "Layer 25 loss with multiplier 11 is 0.5223752151462949\n",
      "Layer 25 loss with multiplier 13 is 0.9087779690189474\n",
      "Layer 26 loss with multiplier 1 is 0.6394148020654057\n",
      "Layer 26 loss with multiplier 3 is 0.5989672977624777\n",
      "Layer 26 loss with multiplier 5 is 0.5826161790017196\n",
      "Layer 26 loss with multiplier 7 is 0.5215146299483603\n",
      "Layer 26 loss with multiplier 9 is 0.41824440619620906\n",
      "Layer 26 loss with multiplier 11 is 0.32099827882960125\n",
      "Layer 26 loss with multiplier 13 is 0.23321858864027384\n",
      "Layer 26 loss with multiplier 15 is 0.2099827882960401\n",
      "Layer 26 loss with multiplier 17 is 0.2977624784853675\n",
      "Layer 26 loss with multiplier 19 is 0.5464716006884648\n",
      "Layer 26 loss with multiplier 21 is 0.8175559380378757\n",
      "Layer 26 loss with multiplier 23 is 0.9716006884681759\n",
      "Layer 27 loss with multiplier 1 is 0.6376936316695364\n",
      "Layer 27 loss with multiplier 3 is 0.5808950086058503\n",
      "Layer 27 loss with multiplier 5 is 0.5309810671256413\n",
      "Layer 27 loss with multiplier 7 is 0.4716006884681532\n",
      "Layer 27 loss with multiplier 9 is 0.4113597246127324\n",
      "Layer 27 loss with multiplier 11 is 0.323580034423405\n",
      "Layer 27 loss with multiplier 13 is 0.31583476764199375\n",
      "Layer 27 loss with multiplier 15 is 0.5456110154905301\n",
      "Layer 27 loss with multiplier 17 is 0.8373493975903724\n",
      "Layer 27 loss with multiplier 19 is 0.9750430292599145\n",
      "Layer 28 loss with multiplier 1 is 0.6316695352839939\n",
      "Layer 28 loss with multiplier 3 is 0.5731497418244386\n",
      "Layer 28 loss with multiplier 5 is 0.5215146299483603\n",
      "Layer 28 loss with multiplier 7 is 0.43889845094663904\n",
      "Layer 28 loss with multiplier 9 is 0.39500860585197534\n",
      "Layer 28 loss with multiplier 11 is 0.30636833046471335\n",
      "Layer 28 loss with multiplier 13 is 0.25817555938037673\n",
      "Layer 28 loss with multiplier 15 is 0.21772805507745135\n",
      "Layer 28 loss with multiplier 17 is 0.25731497418244215\n",
      "Layer 28 loss with multiplier 19 is 0.4259896729776203\n",
      "Layer 28 loss with multiplier 21 is 0.6876075731497454\n",
      "Layer 28 loss with multiplier 23 is 0.8872633390705814\n",
      "Layer 28 loss with multiplier 25 is 0.9827882960413262\n",
      "Layer 29 loss with multiplier 1 is 0.6299483648881247\n",
      "Layer 29 loss with multiplier 3 is 0.6041308089500855\n",
      "Layer 29 loss with multiplier 5 is 0.5757314974182425\n",
      "Layer 29 loss with multiplier 7 is 0.5567986230636804\n",
      "Layer 29 loss with multiplier 9 is 0.5395869191049877\n",
      "Layer 29 loss with multiplier 11 is 0.5068846815834714\n",
      "Layer 29 loss with multiplier 13 is 0.4664371772805457\n",
      "Layer 29 loss with multiplier 15 is 0.4552495697073961\n",
      "Layer 29 loss with multiplier 17 is 0.37091222030980703\n",
      "Layer 29 loss with multiplier 19 is 0.35025817555937705\n",
      "Layer 29 loss with multiplier 21 is 0.3175559380378629\n",
      "Layer 29 loss with multiplier 23 is 0.3769363166953491\n",
      "Layer 29 loss with multiplier 25 is 0.5301204819277067\n",
      "Layer 29 loss with multiplier 27 is 0.691049913941484\n",
      "Layer 29 loss with multiplier 29 is 0.8227194492254836\n",
      "Layer 29 loss with multiplier 31 is 0.9079173838210127\n",
      "Layer 30 loss with multiplier 1 is 0.641135972461275\n",
      "Layer 30 loss with multiplier 3 is 0.6480206540447521\n",
      "Layer 30 loss with multiplier 5 is 0.6256454388984515\n",
      "Layer 30 loss with multiplier 7 is 0.6247848537005168\n",
      "Layer 30 loss with multiplier 9 is 0.6049913941480202\n",
      "Layer 30 loss with multiplier 11 is 0.5920826161790006\n",
      "Layer 30 loss with multiplier 13 is 0.5619621342512883\n",
      "Layer 30 loss with multiplier 15 is 0.5524956970740073\n",
      "Layer 30 loss with multiplier 17 is 0.5619621342512883\n",
      "Layer 30 loss with multiplier 19 is 0.5223752151462949\n",
      "Layer 30 loss with multiplier 21 is 0.5266781411359681\n",
      "Layer 30 loss with multiplier 23 is 0.49311531841651773\n",
      "Layer 30 loss with multiplier 25 is 0.45352839931152694\n",
      "Layer 30 loss with multiplier 27 is 0.4543889845094615\n",
      "Layer 30 loss with multiplier 29 is 0.42340791738381656\n",
      "Layer 30 loss with multiplier 31 is 0.3872633390705641\n",
      "Layer 30 loss with multiplier 33 is 0.38554216867469493\n",
      "Layer 30 loss with multiplier 35 is 0.36919104991393786\n",
      "Layer 30 loss with multiplier 37 is 0.38037865748708743\n",
      "Layer 30 loss with multiplier 39 is 0.4354561101549007\n",
      "Layer 31 loss with multiplier 1 is 0.6342512908777979\n",
      "Layer 31 loss with multiplier 3 is 0.619621342512909\n",
      "Layer 31 loss with multiplier 5 is 0.6161790017211705\n",
      "Layer 31 loss with multiplier 7 is 0.6084337349397587\n",
      "Layer 31 loss with multiplier 9 is 0.6024096385542163\n",
      "Layer 31 loss with multiplier 11 is 0.5654044750430268\n",
      "Layer 31 loss with multiplier 13 is 0.5490533562822687\n",
      "Layer 31 loss with multiplier 15 is 0.5206540447504256\n",
      "Layer 31 loss with multiplier 17 is 0.5172117039586871\n",
      "Layer 31 loss with multiplier 19 is 0.5043029259896675\n",
      "Layer 31 loss with multiplier 21 is 0.469879518072284\n",
      "Layer 31 loss with multiplier 23 is 0.43803786574870446\n",
      "Layer 31 loss with multiplier 25 is 0.43459552495696613\n",
      "Layer 31 loss with multiplier 27 is 0.42943201376935863\n",
      "Layer 31 loss with multiplier 29 is 0.4173838209982745\n",
      "Layer 31 loss with multiplier 31 is 0.4113597246127324\n",
      "Layer 31 loss with multiplier 33 is 0.4061962134251249\n",
      "Layer 31 loss with multiplier 35 is 0.38984509466436784\n",
      "Layer 31 loss with multiplier 37 is 0.34079173838209664\n",
      "Layer 31 loss with multiplier 39 is 0.3571428571428537\n"
     ]
    }
   ],
   "source": [
    "#calculating the loss for each layes\n",
    "#initial_loss = calc_loss_steering_vector(generator, torch.tensor(4096*[0]), (input_list_test, output_list_test), layer=12, iter=len(input_list_test), multiplier=0)\n",
    "print(f\"initial loss is {initial_loss}\")\n",
    "for k in range(16,32):\n",
    "    steering_vec = torch.load(f\"./c_and_n_vectors/vector{k}.pt\")\n",
    "    for multiplier in range(1,40,2):\n",
    "        loss = calc_loss_steering_vector(generator, steering_vec, (input_list_test, output_list_test), layer=k, iter=len(input_list_test), multiplier=multiplier)\n",
    "        print(f\"Layer {k} loss with multiplier {multiplier} is {loss}\")\n",
    "        losses2[f\"{k},{multiplier}\"] = loss\n",
    "        if loss>0.9:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"initial_values2.json\", \"w\") as file:\n",
    "    json.dump(losses2, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([-0.0361, -0.0046, -0.0242,  ..., -0.0271, -0.0303,  0.0280])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ribbe\\AppData\\Local\\Temp\\ipykernel_3580\\3856237667.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vec1 = torch.load(\"./steering_vectors/vector2.pt\")\n",
      "C:\\Users\\Ribbe\\AppData\\Local\\Temp\\ipykernel_3580\\3856237667.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vec2 = torch.load(\"./steering_without_instruct/vector2.pt\")\n"
     ]
    }
   ],
   "source": [
    "vec1 = torch.load(\"./steering_vectors/vector2.pt\")\n",
    "vec2 = torch.load(\"./steering_without_instruct/vector2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ribbe\\AppData\\Local\\Temp\\ipykernel_3580\\3787317746.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vec = torch.load(f\"./steering_without_instruct/vector{k}.pt\")\n"
     ]
    }
   ],
   "source": [
    "for k in range(32):\n",
    "    vec = torch.load(f\"./steering_without_instruct/vector{k}.pt\")\n",
    "    c_vec = vec - torch.mean(vec)\n",
    "    n_c_vec = torch.nn.functional.normalize(c_vec, dim=0)\n",
    "    torch.save(n_c_vec, f\"./c_and_n_vectors/vector{k}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ribbe\\AppData\\Local\\Temp\\ipykernel_24320\\685555240.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  steering_vec = 14.5*torch.load(\"./steering_without_instruct/n_vector26.pt\")\n"
     ]
    }
   ],
   "source": [
    "#seeing if the general reasoning abilities improve\n",
    "steering_vec = 14.5*torch.load(\"./steering_without_instruct/n_vector26.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.change_activation_vector(steering_vec)\n",
    "generator.change_activation_layer(26)\n",
    "value, activations = generator.chat_completion([input_list_test[1]],\n",
    "                                   max_gen_len=max_gen_len,\n",
    "                                   top_p=top_p,\n",
    "                                   temperature=temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generation': {'role': 'assistant',\n",
       "   'content': 'F.\\n\\nThe given statistical relations among A, B, C, D, and E suggest a complex network of correlations and independence. However, the hypothesis that B causes something else which causes D is not supported by the given information.\\n\\nA and D are independent given B and C, and also given B, C, and E. This suggests that A and D are not directly related, and that B and C, or B, C, and E, are sufficient to explain the independence of A and D.\\n\\nFurthermore, D and E are independent given A, B, and C, and also given A and C. This suggests that D and E are not directly related, and that A, B, and C, or A and C, are sufficient to explain the independence of D and E.\\n\\nThere is no information that suggests B causes something else which causes D. In fact, B and E are independent given A and C, and also given A, C, and D. This suggests that B and E are not directly related, and that A and C, or A, C, and D, are sufficient to explain the independence of B and E.\\n\\nTherefore, the hypothesis that B causes something else which causes D is not supported by the given information, and is likely to be false.'}}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_indices = [i for i,x in enumerate(output_list_test) if x==\"0\"]\n",
    "positive_indices = [i for i,x in enumerate(output_list_test) if x==\"1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_indices_train =  [i for i,x in enumerate(output_list_train) if x==\"0\" and i<=1094]\n",
    "positive_indices_train =  [i for i,x in enumerate(output_list_train) if x==\"1\" and i<=7865]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_positive_list_train = [input_list_train[k] for k in positive_indices_train]\n",
    "input_negative_list_train = [input_list_train[k] for k in negative_indices_train]\n",
    "input_list_train_short = input_positive_list_train + input_negative_list_train\n",
    "output_list_train_short = 1000*[\"1\"] + 1000*[\"0\"]\n",
    "print(len(input_list_train_short) == len(output_list_train_short))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss is 0.5539999999999944\n",
      "Training loss is 0.5534999999999944\n",
      "Training loss is 0.548499999999995\n",
      "Training loss is 0.5509999999999947\n",
      "Training loss is 0.5489999999999949\n",
      "Training loss is 0.5624999999999934\n",
      "Training loss is 0.5504999999999948\n",
      "Training loss is 0.5599999999999937\n",
      "Training loss is 0.5524999999999946\n",
      "Training loss is 0.5534999999999944\n",
      "Training loss is 0.5489999999999949\n",
      "Training loss is 0.5549999999999943\n",
      "Training loss is 0.557999999999994\n",
      "Training loss is 0.5499999999999948\n",
      "Training loss is 0.5604999999999937\n",
      "Training loss is 0.5519999999999946\n",
      "Training loss is 0.557499999999994\n",
      "Training loss is 0.5524999999999946\n",
      "Training loss is 0.5474999999999951\n",
      "Training loss is 0.5489999999999949\n",
      "Training loss is 0.5619999999999935\n",
      "Training loss is 0.5554999999999942\n",
      "Training loss is 0.5529999999999945\n",
      "Training loss is 0.5534999999999944\n",
      "Training loss is 0.547999999999995\n",
      "Training loss is 0.5524999999999946\n",
      "Training loss is 0.5499999999999948\n",
      "Training loss is 0.5519999999999946\n",
      "Training loss is 0.547999999999995\n",
      "Training loss is 0.5489999999999949\n",
      "Training loss is 0.5524999999999946\n",
      "Training loss is 0.556999999999994\n",
      "Average loss is 0.5530937499999946\n"
     ]
    }
   ],
   "source": [
    "#extracting a steering vector for every layer while keeping the amount of positive and negative vectors constant\n",
    "average_loss = 0\n",
    "for k in range(32):\n",
    "    steering_vec, loss = extracting_steering_vector(generator, (input_list_train_short, output_list_train_short),layer=k, iter=2000)\n",
    "    steering_vec = torch.nn.functional.normalize(steering_vec, dim=0)\n",
    "    torch.save(steering_vec, f\"./even_vectors/vector{k}.pt\")\n",
    "    print(f\"Training loss is {loss}\")\n",
    "    average_loss += 1/len(generator.model.layers)*loss\n",
    "print(f\"Average loss is {average_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.change_activation_vector(torch.tensor(4096*[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'Answer with T if you believe the following statement is correct, answer with F if you believe it is false, do not give any reasoning.'},\n",
       " {'role': 'system',\n",
       "  'content': 'You are a helpful assistant in casual reasoning, is the following reasoning true?'},\n",
       " {'role': 'user',\n",
       "  'content': 'Premise: Suppose there is a closed system of 4 variables, A, B, C and D. All the statistical relations among these 4 variables are as follows: A correlates with C. A correlates with D. B correlates with C. C correlates with D. However, A is independent of B. A and B are independent given D. B is independent of D. B and D are independent given A. B and D are independent given A and C. C and D are independent given A. C and D are independent given A and B.\\nHypothesis: D directly causes B.'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_list_train_short[1203]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value, vec = generator.chat_completion([input_list_train_short[1203]], temperature=temperature, top_p=top_p, max_gen_len=1023)\n",
    "output_list_train_short[1203]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses2 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loss is 0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ribbe\\AppData\\Local\\Temp\\ipykernel_18468\\965858657.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  steering_vec = torch.load(f\"./even_vectors/vector{k}.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 16 loss with multiplier 1 is 0.6936316695352879\n",
      "Layer 16 loss with multiplier 3 is 0.650602409638556\n",
      "Layer 16 loss with multiplier 5 is 0.6058519793459548\n",
      "Layer 16 loss with multiplier 7 is 0.5895008605851967\n",
      "Layer 16 loss with multiplier 9 is 0.6635111876075755\n",
      "Layer 16 loss with multiplier 11 is 0.7538726333907125\n",
      "Layer 16 loss with multiplier 13 is 0.8080895008605947\n",
      "Layer 16 loss with multiplier 15 is 0.8218588640275489\n",
      "Layer 16 loss with multiplier 17 is 0.9251290877797055\n",
      "Layer 17 loss with multiplier 1 is 0.6712564543889873\n",
      "Layer 17 loss with multiplier 3 is 0.638554216867471\n",
      "Layer 17 loss with multiplier 5 is 0.638554216867471\n",
      "Layer 17 loss with multiplier 7 is 0.6626506024096409\n",
      "Layer 17 loss with multiplier 9 is 0.719449225473327\n",
      "Layer 17 loss with multiplier 11 is 0.7547332185886472\n",
      "Layer 17 loss with multiplier 13 is 0.7934595524957059\n",
      "Layer 17 loss with multiplier 15 is 0.8209982788296143\n",
      "Layer 17 loss with multiplier 17 is 0.8330464716006992\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m steering_vec \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./even_vectors/vector\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m multiplier \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m25\u001b[39m,\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m----> 7\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_loss_steering_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteering_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_list_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_list_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_list_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiplier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultiplier\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m loss with multiplier \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmultiplier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m     losses2[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmultiplier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\eval\\eval.py:55\u001b[0m, in \u001b[0;36mcalc_loss_steering_vector\u001b[1;34m(generator, steering_vec, data, layer, iter, multiplier)\u001b[0m\n\u001b[0;32m     53\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28miter\u001b[39m):\n\u001b[1;32m---> 55\u001b[0m     value, activations \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_list_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mmax_gen_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_gen_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     60\u001b[0m         value2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\llama\\generation.py:325\u001b[0m, in \u001b[0;36mLlama.chat_completion\u001b[1;34m(self, dialogs, temperature, top_p, max_gen_len, logprobs)\u001b[0m\n\u001b[0;32m    320\u001b[0m     max_gen_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mmax_seq_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    322\u001b[0m prompt_tokens \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformatter\u001b[38;5;241m.\u001b[39mencode_dialog_prompt(dialog) \u001b[38;5;28;01mfor\u001b[39;00m dialog \u001b[38;5;129;01min\u001b[39;00m dialogs\n\u001b[0;32m    324\u001b[0m ]\n\u001b[1;32m--> 325\u001b[0m generation_tokens, generation_logprobs, activations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_gen_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_gen_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logprobs:\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    334\u001b[0m         {\n\u001b[0;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m t, logprobs_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(generation_tokens, generation_logprobs)\n\u001b[0;32m    343\u001b[0m     ], activations\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\llama\\generation.py:181\u001b[0m, in \u001b[0;36mLlama.generate\u001b[1;34m(self, prompt_tokens, max_gen_len, temperature, top_p, logprobs, echo)\u001b[0m\n\u001b[0;32m    179\u001b[0m stop_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mstop_tokens))\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cur_pos \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(min_prompt_len, total_len):\n\u001b[1;32m--> 181\u001b[0m     logits, activation_vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_pos\u001b[49m\u001b[43m:\u001b[49m\u001b[43mcur_pos\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_pos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m temperature \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    183\u001b[0m         probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m temperature, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\llama\\model.py:309\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, tokens, start_pos)\u001b[0m\n\u001b[0;32m    307\u001b[0m activation_tensor \u001b[38;5;241m=\u001b[39m activation_tensor\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    308\u001b[0m activation_vector \u001b[38;5;241m=\u001b[39m activation_tensor\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mequal(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madding_activation_vector, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcuda()):\n\u001b[0;32m    310\u001b[0m     steering_vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madding_activation_vector\u001b[38;5;241m.\u001b[39mrepeat(h\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    311\u001b[0m     steering_vector \u001b[38;5;241m=\u001b[39m steering_vector\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#calculating the loss for each layes\n",
    "#initial_loss = calc_loss_steering_vector(generator, torch.tensor(4096*[0]), (input_list_test, output_list_test), layer=12, iter=len(input_list_test), multiplier=0)\n",
    "print(f\"initial loss is {0.68}\")\n",
    "for k in range(16,32):\n",
    "    steering_vec = torch.load(f\"./even_vectors/vector{k}.pt\")\n",
    "    for multiplier in range(1,25,2):\n",
    "        loss = calc_loss_steering_vector(generator, steering_vec, (input_list_test, output_list_test), layer=k, iter=len(input_list_test), multiplier=multiplier)\n",
    "        print(f\"Layer {k} loss with multiplier {multiplier} is {loss}\")\n",
    "        losses2[f\"{k},{multiplier}\"] = loss\n",
    "        if loss>0.85:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#even test set\n",
    "\n",
    "negative_indices_test  = [i for i,x in enumerate(output_list_test) if x==\"0\" and i<184]\n",
    "positive_indices_test = [i for i,x in enumerate(output_list_test) if x==\"1\" ]\n",
    "\n",
    "input_positive_list_test = [input_list_test[k] for k in positive_indices_test]\n",
    "input_negative_list_test = [input_list_test[k] for k in negative_indices_test]\n",
    "\n",
    "input_list_test_short = input_positive_list_test + input_negative_list_test\n",
    "output_list_test_short = 180*[1] + 180*[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ribbe\\AppData\\Local\\Temp\\ipykernel_18468\\1235680360.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  steering_vec = torch.load(f\"./even_vectors/vector{k}.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 16 loss with multiplier 1 is 0.3444444444444437\n",
      "Layer 16 loss with multiplier 3 is 0.30833333333333274\n",
      "Layer 16 loss with multiplier 5 is 0.27222222222222175\n",
      "Layer 16 loss with multiplier 7 is 0.37499999999999917\n",
      "Layer 16 loss with multiplier 9 is 0.4055555555555546\n",
      "Layer 16 loss with multiplier 11 is 0.40277777777777685\n",
      "Layer 16 loss with multiplier 13 is 0.44722222222222113\n",
      "Layer 16 loss with multiplier 15 is 0.5277777777777765\n",
      "Layer 16 loss with multiplier 17 is 0.7861111111111089\n",
      "Layer 16 loss with multiplier 19 is 0.9194444444444417\n",
      "Layer 17 loss with multiplier 1 is 0.31666666666666604\n",
      "Layer 17 loss with multiplier 3 is 0.34999999999999926\n",
      "Layer 17 loss with multiplier 5 is 0.30555555555555497\n",
      "Layer 17 loss with multiplier 7 is 0.36111111111111033\n",
      "Layer 17 loss with multiplier 9 is 0.3916666666666658\n",
      "Layer 17 loss with multiplier 11 is 0.41944444444444345\n",
      "Layer 17 loss with multiplier 13 is 0.42777777777777676\n",
      "Layer 17 loss with multiplier 15 is 0.43611111111111006\n",
      "Layer 17 loss with multiplier 17 is 0.4666666666666655\n",
      "Layer 17 loss with multiplier 19 is 0.47222222222222104\n",
      "Layer 17 loss with multiplier 21 is 0.4999999999999987\n",
      "Layer 17 loss with multiplier 23 is 0.5083333333333321\n",
      "Layer 18 loss with multiplier 1 is 0.32499999999999934\n",
      "Layer 18 loss with multiplier 3 is 0.3138888888888883\n",
      "Layer 18 loss with multiplier 5 is 0.28055555555555506\n",
      "Layer 18 loss with multiplier 7 is 0.29166666666666613\n",
      "Layer 18 loss with multiplier 9 is 0.3222222222222216\n",
      "Layer 18 loss with multiplier 11 is 0.3361111111111104\n",
      "Layer 18 loss with multiplier 13 is 0.3472222222222215\n",
      "Layer 18 loss with multiplier 15 is 0.38333333333333247\n",
      "Layer 18 loss with multiplier 17 is 0.38611111111111024\n",
      "Layer 18 loss with multiplier 19 is 0.4499999999999989\n",
      "Layer 18 loss with multiplier 21 is 0.44444444444444337\n",
      "Layer 18 loss with multiplier 23 is 0.5138888888888876\n",
      "Layer 19 loss with multiplier 1 is 0.3027777777777772\n",
      "Layer 19 loss with multiplier 3 is 0.29722222222222167\n",
      "Layer 19 loss with multiplier 5 is 0.34166666666666595\n",
      "Layer 19 loss with multiplier 7 is 0.3361111111111104\n",
      "Layer 19 loss with multiplier 9 is 0.34166666666666595\n",
      "Layer 19 loss with multiplier 11 is 0.3722222222222214\n",
      "Layer 19 loss with multiplier 13 is 0.352777777777777\n",
      "Layer 19 loss with multiplier 15 is 0.38333333333333247\n",
      "Layer 19 loss with multiplier 17 is 0.3388888888888882\n",
      "Layer 19 loss with multiplier 19 is 0.3999999999999991\n",
      "Layer 19 loss with multiplier 21 is 0.36666666666666586\n",
      "Layer 19 loss with multiplier 23 is 0.38611111111111024\n",
      "Layer 20 loss with multiplier 1 is 0.34166666666666595\n",
      "Layer 20 loss with multiplier 3 is 0.36111111111111033\n",
      "Layer 20 loss with multiplier 5 is 0.38333333333333247\n",
      "Layer 20 loss with multiplier 7 is 0.37499999999999917\n",
      "Layer 20 loss with multiplier 9 is 0.3916666666666658\n",
      "Layer 20 loss with multiplier 11 is 0.42777777777777676\n",
      "Layer 20 loss with multiplier 13 is 0.41111111111111015\n",
      "Layer 20 loss with multiplier 15 is 0.40277777777777685\n",
      "Layer 20 loss with multiplier 17 is 0.4861111111111099\n",
      "Layer 20 loss with multiplier 19 is 0.4944444444444432\n",
      "Layer 20 loss with multiplier 21 is 0.5527777777777764\n",
      "Layer 20 loss with multiplier 23 is 0.6722222222222204\n",
      "Layer 21 loss with multiplier 1 is 0.33333333333333265\n",
      "Layer 21 loss with multiplier 3 is 0.3027777777777772\n",
      "Layer 21 loss with multiplier 5 is 0.3444444444444437\n",
      "Layer 21 loss with multiplier 7 is 0.36944444444444363\n",
      "Layer 21 loss with multiplier 9 is 0.41111111111111015\n",
      "Layer 21 loss with multiplier 11 is 0.4055555555555546\n",
      "Layer 21 loss with multiplier 13 is 0.35833333333333256\n",
      "Layer 21 loss with multiplier 15 is 0.45277777777777667\n",
      "Layer 21 loss with multiplier 17 is 0.4777777777777766\n",
      "Layer 21 loss with multiplier 19 is 0.580555555555554\n",
      "Layer 21 loss with multiplier 21 is 0.6222222222222206\n",
      "Layer 21 loss with multiplier 23 is 0.7194444444444424\n",
      "Layer 22 loss with multiplier 1 is 0.3222222222222216\n",
      "Layer 22 loss with multiplier 3 is 0.3388888888888882\n",
      "Layer 22 loss with multiplier 5 is 0.36666666666666586\n",
      "Layer 22 loss with multiplier 7 is 0.3555555555555548\n",
      "Layer 22 loss with multiplier 9 is 0.3722222222222214\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m steering_vec \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./even_vectors/vector\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m multiplier \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m25\u001b[39m,\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m----> 4\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_loss_steering_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteering_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_list_test_short\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_list_test_short\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m360\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiplier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultiplier\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m loss with multiplier \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmultiplier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     losses2[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmultiplier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\eval\\eval.py:55\u001b[0m, in \u001b[0;36mcalc_loss_steering_vector\u001b[1;34m(generator, steering_vec, data, layer, iter, multiplier)\u001b[0m\n\u001b[0;32m     53\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28miter\u001b[39m):\n\u001b[1;32m---> 55\u001b[0m     value, activations \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_list_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mmax_gen_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_gen_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     60\u001b[0m         value2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\llama\\generation.py:325\u001b[0m, in \u001b[0;36mLlama.chat_completion\u001b[1;34m(self, dialogs, temperature, top_p, max_gen_len, logprobs)\u001b[0m\n\u001b[0;32m    320\u001b[0m     max_gen_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mmax_seq_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    322\u001b[0m prompt_tokens \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformatter\u001b[38;5;241m.\u001b[39mencode_dialog_prompt(dialog) \u001b[38;5;28;01mfor\u001b[39;00m dialog \u001b[38;5;129;01min\u001b[39;00m dialogs\n\u001b[0;32m    324\u001b[0m ]\n\u001b[1;32m--> 325\u001b[0m generation_tokens, generation_logprobs, activations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_gen_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_gen_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logprobs:\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    334\u001b[0m         {\n\u001b[0;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m t, logprobs_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(generation_tokens, generation_logprobs)\n\u001b[0;32m    343\u001b[0m     ], activations\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\llama\\generation.py:181\u001b[0m, in \u001b[0;36mLlama.generate\u001b[1;34m(self, prompt_tokens, max_gen_len, temperature, top_p, logprobs, echo)\u001b[0m\n\u001b[0;32m    179\u001b[0m stop_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mstop_tokens))\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cur_pos \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(min_prompt_len, total_len):\n\u001b[1;32m--> 181\u001b[0m     logits, activation_vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_pos\u001b[49m\u001b[43m:\u001b[49m\u001b[43mcur_pos\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_pos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m temperature \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    183\u001b[0m         probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m temperature, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\llama\\model.py:304\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, tokens, start_pos)\u001b[0m\n\u001b[0;32m    299\u001b[0m     mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhstack(\n\u001b[0;32m    300\u001b[0m         [torch\u001b[38;5;241m.\u001b[39mzeros((seqlen, start_pos), device\u001b[38;5;241m=\u001b[39mtokens\u001b[38;5;241m.\u001b[39mdevice), mask]\n\u001b[0;32m    301\u001b[0m     )\u001b[38;5;241m.\u001b[39mtype_as(h)\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[1;32m--> 304\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_layer_n \u001b[38;5;241m==\u001b[39m k:\n\u001b[0;32m    306\u001b[0m         activation_tensor \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mclone()\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\llama\\model.py:246\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x, start_pos, freqs_cis, mask)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    241\u001b[0m     x: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    244\u001b[0m     mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[0;32m    245\u001b[0m ):\n\u001b[1;32m--> 246\u001b[0m     h \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    247\u001b[0m     out \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn_norm(h))\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\llama\\model.py:154\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, x, start_pos, freqs_cis, mask)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    148\u001b[0m     x: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    151\u001b[0m     mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[0;32m    152\u001b[0m ):\n\u001b[0;32m    153\u001b[0m     bsz, seqlen, _ \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m--> 154\u001b[0m     xq, xk, xv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwk(x), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwv(x)\n\u001b[0;32m    156\u001b[0m     xq \u001b[38;5;241m=\u001b[39m xq\u001b[38;5;241m.\u001b[39mview(bsz, seqlen, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_local_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[0;32m    157\u001b[0m     xk \u001b[38;5;241m=\u001b[39m xk\u001b[38;5;241m.\u001b[39mview(bsz, seqlen, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_local_kv_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\fairscale\\nn\\model_parallel\\layers.py:290\u001b[0m, in \u001b[0;36mColumnParallelLinear.forward\u001b[1;34m(self, input_)\u001b[0m\n\u001b[0;32m    288\u001b[0m input_parallel \u001b[38;5;241m=\u001b[39m copy_to_model_parallel_region(input_)\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# Matrix multiply.\u001b[39;00m\n\u001b[1;32m--> 290\u001b[0m output_parallel \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_parallel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_output:\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;66;03m# All-gather across the partitions.\u001b[39;00m\n\u001b[0;32m    293\u001b[0m     output \u001b[38;5;241m=\u001b[39m gather_from_model_parallel_region(output_parallel)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for k in range(16,32):\n",
    "    steering_vec = torch.load(f\"./even_vectors/vector{k}.pt\")\n",
    "    for multiplier in range(1,25,2):\n",
    "        loss = calc_loss_steering_vector(generator, steering_vec, (input_list_test_short, output_list_test_short), layer=k, iter=360, multiplier=multiplier)\n",
    "        print(f\"Layer {k} loss with multiplier {multiplier} is {loss}\")\n",
    "        losses2[f\"{k},{multiplier}\"] = loss\n",
    "        if loss>0.85:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 26 loss with multiplier 0 is 0.2944444444444439\n"
     ]
    }
   ],
   "source": [
    "loss = calc_loss_steering_vector(generator, steering_vec, (input_list_test_short, output_list_test_short), layer=26, iter=360, multiplier=0)\n",
    "print(f\"Layer {26} loss with multiplier {0} is {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Coding_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
