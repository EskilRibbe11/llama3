{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "import fire\n",
    "\n",
    "from llama import Dialog, Llama\n",
    "import torch.distributed as dist\n",
    "import torch\n",
    "import os\n",
    "from data_casual import output_list_train, input_list_train, input_list_test, output_list_test\n",
    "from eval import extracting_steering_vector, calc_loss_steering_vector\n",
    "\n",
    "ckpt_dir = \"./\"\n",
    "tokenizer_path = \"./tokenizer.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\llama\\generation.py:98: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
      "c:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\torch\\__init__.py:1144: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\tensor\\python_tensor.cpp:434.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in 258.36 seconds\n",
      "<llama.generation.Llama object at 0x00000204C3FB39D0>\n"
     ]
    }
   ],
   "source": [
    "generator = Llama.build(ckpt_dir=ckpt_dir, tokenizer_path=tokenizer_path, max_seq_len= 1024, max_batch_size= 4, activation=True, activation_layer=12)\n",
    "print(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.6\n",
    "top_p = 0.9\n",
    "max_seq_len = 1024\n",
    "max_batch_size = 4\n",
    "max_gen_len= generator.model.params.max_seq_len - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting a steering vector for every layer\n",
    "average_loss = 0\n",
    "for k in range(len(generator.model.layers)):\n",
    "    steering_vector, loss = extracting_steering_vector(generator, (input_list_train, output_list_train),layer=k, iter=2000)\n",
    "    torch.save(steering_vector, f\"./steering_vectors/vector{k}.pt\")\n",
    "    print(f\"Training loss is {loss}\")\n",
    "    average_loss += 1/len(generator.model.layers)*loss\n",
    "print(f\"Average loss is {average_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_loss = 0.688468\n",
    "losses = {}\n",
    "losses[\"initial_loss\"] = initial_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loss is 0.688468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ribbe\\AppData\\Local\\Temp\\ipykernel_472\\3511317914.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  steering_vec = torch.load(f\"./steering_vectors/n_vector{k}.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 16 loss is 0.5989672977624777\n",
      "Layer 17 loss is 0.6970740103270264\n",
      "Layer 18 loss is 0.6755593803786605\n",
      "Layer 19 loss is 0.6187607573149744\n",
      "Layer 20 loss is 0.6222030981067129\n",
      "Layer 21 loss is 0.641135972461275\n",
      "Layer 22 loss is 0.6652323580034448\n",
      "Layer 23 loss is 0.6901893287435493\n",
      "Layer 24 loss is 0.6592082616179024\n",
      "Layer 25 loss is 0.6962134251290918\n",
      "Layer 26 loss is 0.681583476764203\n",
      "Layer 27 loss is 0.650602409638556\n",
      "Layer 28 loss is 0.6609294320137716\n",
      "Layer 29 loss is 0.6781411359724644\n",
      "Layer 30 loss is 0.669535283993118\n",
      "Layer 31 loss is 0.6841652323580069\n"
     ]
    }
   ],
   "source": [
    "#calculating the loss for each layes\n",
    "print(f\"initial loss is {initial_loss}\")\n",
    "for k in range(16,32):\n",
    "    multiplier = 1\n",
    "    steering_vec = torch.load(f\"./steering_vectors/n_vector{k}.pt\")\n",
    "    loss = calc_loss_steering_vector(generator, steering_vec, (input_list_test, output_list_test), layer=k, iter=len(input_list_test), multiplier=multiplier)\n",
    "    print(f\"Layer {k} loss is {loss}\")\n",
    "    losses[k] = loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loss is 0.688468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ribbe\\AppData\\Local\\Temp\\ipykernel_472\\2500657696.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  steering_vec = torch.load(f\"./steering_vectors/n_vector{k}.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 16 loss is 0.560240963855419\n",
      "Layer 17 loss is 0.7117039586919153\n",
      "Layer 18 loss is 0.6273666092943208\n",
      "Layer 19 loss is 0.5671256454388961\n",
      "Layer 20 loss is 0.6351118760757325\n",
      "Layer 21 loss is 0.5920826161790006\n",
      "Layer 22 loss is 0.6781411359724644\n",
      "Layer 23 loss is 0.7203098106712617\n",
      "Layer 24 loss is 0.6583476764199677\n",
      "Layer 25 loss is 0.6824440619621376\n",
      "Layer 26 loss is 0.68846815834768\n",
      "Layer 27 loss is 0.6540447504302945\n",
      "Layer 28 loss is 0.6531841652323599\n",
      "Layer 29 loss is 0.6686746987951834\n",
      "Layer 30 loss is 0.669535283993118\n",
      "Layer 31 loss is 0.679001721170399\n"
     ]
    }
   ],
   "source": [
    "losses2 = {}\n",
    "print(f\"initial loss is {initial_loss}\")\n",
    "for k in range(16,32):\n",
    "    multiplier = 2\n",
    "    steering_vec = torch.load(f\"./steering_vectors/n_vector{k}.pt\")\n",
    "    loss = calc_loss_steering_vector(generator, steering_vec, (input_list_test, output_list_test), layer=k, iter=len(input_list_test), multiplier=multiplier)\n",
    "    print(f\"Layer {k} loss is {loss}\")\n",
    "    losses2[k] = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loss is 0.688468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ribbe\\AppData\\Local\\Temp\\ipykernel_472\\3711278118.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  steering_vec = torch.load(f\"./steering_vectors/n_vector{k}.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 loss is 0.8433734939759149\n",
      "Layer 1 loss is 0.6265060240963861\n",
      "Layer 2 loss is 0.6256454388984515\n",
      "Layer 3 loss is 0.7254733218588695\n",
      "Layer 4 loss is 0.7951807228915752\n",
      "Layer 5 loss is 0.7891566265060327\n",
      "Layer 6 loss is 0.7211703958691963\n",
      "Layer 7 loss is 0.7728055077452746\n",
      "Layer 8 loss is 0.7865748709122288\n",
      "Layer 9 loss is 0.6531841652323599\n",
      "Layer 10 loss is 0.7211703958691963\n",
      "Layer 11 loss is 0.5808950086058503\n",
      "Layer 12 loss is 0.6970740103270264\n",
      "Layer 13 loss is 0.6282271944922554\n",
      "Layer 14 loss is 0.6153184165232358\n",
      "Layer 15 loss is 0.5826161790017196\n",
      "Layer 16 loss is 0.6316695352839939\n",
      "Layer 17 loss is 0.6996557659208303\n",
      "Layer 18 loss is 0.6970740103270264\n",
      "Layer 19 loss is 0.6127366609294319\n",
      "Layer 20 loss is 0.638554216867471\n",
      "Layer 21 loss is 0.641135972461275\n",
      "Layer 22 loss is 0.6669535283993141\n",
      "Layer 23 loss is 0.6979345955249611\n",
      "Layer 24 loss is 0.6703958691910527\n",
      "Layer 25 loss is 0.6807228915662683\n",
      "Layer 26 loss is 0.6583476764199677\n",
      "Layer 27 loss is 0.6686746987951834\n",
      "Layer 28 loss is 0.6807228915662683\n",
      "Layer 29 loss is 0.6824440619621376\n",
      "Layer 30 loss is 0.6781411359724644\n",
      "Layer 31 loss is 0.6798623063683337\n"
     ]
    }
   ],
   "source": [
    "losses05 = {}\n",
    "print(f\"initial loss is {initial_loss}\")\n",
    "for k in range(32):\n",
    "    multiplier = 0.5\n",
    "    steering_vec = torch.load(f\"./steering_vectors/n_vector{k}.pt\")\n",
    "    loss = calc_loss_steering_vector(generator, steering_vec, (input_list_test, output_list_test), layer=k, iter=len(input_list_test), multiplier=multiplier)\n",
    "    print(f\"Layer {k} loss is {loss}\")\n",
    "    losses05[k] = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ribbe\\AppData\\Local\\Temp\\ipykernel_472\\1863661030.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  steering_vec = torch.load(f\"./steering_vectors/n_vector{k}.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 loss is 0.6721170395869219\n",
      "Layer 1 loss is 0.6807228915662683\n",
      "Layer 2 loss is 0.6643717728055102\n",
      "Layer 3 loss is 0.6841652323580069\n",
      "Layer 4 loss is 0.6746987951807258\n",
      "Layer 5 loss is 0.6996557659208303\n",
      "Layer 6 loss is 0.6755593803786605\n",
      "Layer 7 loss is 0.6729776247848566\n",
      "Layer 8 loss is 0.6712564543889873\n",
      "Layer 9 loss is 0.679001721170399\n",
      "Layer 10 loss is 0.681583476764203\n",
      "Layer 11 loss is 0.6944922547332225\n",
      "Layer 12 loss is 0.6781411359724644\n",
      "Layer 13 loss is 0.6678141135972487\n",
      "Layer 14 loss is 0.6936316695352879\n",
      "Layer 15 loss is 0.669535283993118\n",
      "Layer 16 loss is 0.6652323580034448\n",
      "Layer 17 loss is 0.6833046471600722\n",
      "Layer 18 loss is 0.6781411359724644\n",
      "Layer 19 loss is 0.6669535283993141\n",
      "Layer 20 loss is 0.6738382099827912\n",
      "Layer 21 loss is 0.6824440619621376\n",
      "Layer 22 loss is 0.6686746987951834\n",
      "Layer 23 loss is 0.6514629948364906\n",
      "Layer 24 loss is 0.6721170395869219\n",
      "Layer 25 loss is 0.6781411359724644\n",
      "Layer 26 loss is 0.6901893287435493\n",
      "Layer 27 loss is 0.6566265060240984\n",
      "Layer 28 loss is 0.6798623063683337\n",
      "Layer 29 loss is 0.669535283993118\n",
      "Layer 30 loss is 0.6772805507745298\n",
      "Layer 31 loss is 0.6592082616179024\n"
     ]
    }
   ],
   "source": [
    "inital_losses = {}\n",
    "for k in range(32):\n",
    "    multiplier = 0\n",
    "    steering_vec = torch.load(f\"./steering_vectors/n_vector{k}.pt\")\n",
    "    loss = calc_loss_steering_vector(generator, steering_vec, (input_list_test, output_list_test), layer=k, iter=len(input_list_test), multiplier=multiplier)\n",
    "    print(f\"Layer {k} loss is {loss}\")\n",
    "    losses05[k] = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loss is 0.688468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ribbe\\AppData\\Local\\Temp\\ipykernel_472\\2679041917.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  steering_vec = torch.load(f\"./steering_vectors/n_vector{k}.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 16 loss is 0.5619621342512883\n",
      "Layer 17 loss is 0.7874354561101634\n",
      "Layer 18 loss is 0.6893287435456147\n",
      "Layer 19 loss is 0.5697074010327\n",
      "Layer 20 loss is 0.6084337349397587\n",
      "Layer 21 loss is 0.5619621342512883\n",
      "Layer 22 loss is 0.6919104991394186\n",
      "Layer 23 loss is 0.6669535283993141\n",
      "Layer 24 loss is 0.6084337349397587\n",
      "Layer 25 loss is 0.6609294320137716\n",
      "Layer 26 loss is 0.6643717728055102\n",
      "Layer 27 loss is 0.5938037865748699\n",
      "Layer 28 loss is 0.6179001721170397\n",
      "Layer 29 loss is 0.6445783132530135\n",
      "Layer 30 loss is 0.6609294320137716\n",
      "Layer 31 loss is 0.6919104991394186\n"
     ]
    }
   ],
   "source": [
    "losses5 = {}\n",
    "print(f\"initial loss is {initial_loss}\")\n",
    "for k in range(16,32):\n",
    "    multiplier = 5\n",
    "    steering_vec = torch.load(f\"./steering_vectors/n_vector{k}.pt\")\n",
    "    loss = calc_loss_steering_vector(generator, steering_vec, (input_list_test, output_list_test), layer=k, iter=len(input_list_test), multiplier=multiplier)\n",
    "    print(f\"Layer {k} loss is {loss}\")\n",
    "    losses5[k] = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loss is 0.688468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ribbe\\AppData\\Local\\Temp\\ipykernel_472\\3968580216.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  steering_vec = torch.load(f\"./steering_vectors/n_vector{k}.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 24 loss is 0.5378657487091184\n",
      "Layer 25 loss is 0.6325301204819286\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "steering vec too large",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m multiplier \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      5\u001b[0m steering_vec \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./steering_vectors/n_vector\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_loss_steering_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteering_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_list_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_list_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_list_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiplier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultiplier\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m loss is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m losses10[k] \u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\eval\\eval.py:59\u001b[0m, in \u001b[0;36mcalc_loss_steering_vector\u001b[1;34m(generator, steering_vec, data, layer, iter, multiplier)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28miter\u001b[39m):\n\u001b[0;32m     55\u001b[0m     value, activations \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39mchat_completion([input_list_test[sample]],\n\u001b[0;32m     56\u001b[0m                                       max_gen_len\u001b[38;5;241m=\u001b[39mmax_gen_len,\n\u001b[0;32m     57\u001b[0m                                        top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m     58\u001b[0m                                        temperature\u001b[38;5;241m=\u001b[39mtemperature)\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m value[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m value[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteering vec too large\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     60\u001b[0m     value2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m value[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     61\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;28miter\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mabs\u001b[39m(\u001b[38;5;28mint\u001b[39m(output_list_test[sample])\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mint\u001b[39m(value2))\n",
      "\u001b[1;31mAssertionError\u001b[0m: steering vec too large"
     ]
    }
   ],
   "source": [
    "losses10 = {}\n",
    "print(f\"initial loss is {initial_loss}\")\n",
    "for k in range(24,32):\n",
    "    multiplier = 10\n",
    "    steering_vec = torch.load(f\"./steering_vectors/n_vector{k}.pt\")\n",
    "    loss = calc_loss_steering_vector(generator, steering_vec, (input_list_test, output_list_test), layer=k, iter=len(input_list_test), multiplier=multiplier)\n",
    "    print(f\"Layer {k} loss is {loss}\")\n",
    "    losses10[k] = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ribbe\\AppData\\Local\\Temp\\ipykernel_472\\167794318.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  steering_vec = torch.load(f\"./steering_vectors/n_vector28.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm 26 loss for layer 28 is 0.6712564543889873\n",
      "norm 26 loss for layer 28 is 0.6359724612736671\n",
      "norm 26 loss for layer 28 is 0.6316695352839939\n",
      "norm 26 loss for layer 28 is 0.6282271944922554\n",
      "norm 26 loss for layer 28 is 0.6213425129087783\n",
      "norm 26 loss for layer 28 is 0.6170395869191051\n",
      "norm 26 loss for layer 28 is 0.6092943201376934\n",
      "norm 26 loss for layer 28 is 0.6015490533562816\n",
      "norm 26 loss for layer 28 is 0.5955249569707391\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "steering vec too large",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m multiplier \u001b[38;5;241m=\u001b[39m mult\n\u001b[0;32m      5\u001b[0m steering_vec \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./steering_vectors/n_vector28.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_loss_steering_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteering_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_list_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_list_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_list_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiplier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultiplier\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnorm \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m loss for layer 28 is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m loss_layer28[mult] \u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\eval\\eval.py:59\u001b[0m, in \u001b[0;36mcalc_loss_steering_vector\u001b[1;34m(generator, steering_vec, data, layer, iter, multiplier)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28miter\u001b[39m):\n\u001b[0;32m     55\u001b[0m     value, activations \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39mchat_completion([input_list_test[sample]],\n\u001b[0;32m     56\u001b[0m                                       max_gen_len\u001b[38;5;241m=\u001b[39mmax_gen_len,\n\u001b[0;32m     57\u001b[0m                                        top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m     58\u001b[0m                                        temperature\u001b[38;5;241m=\u001b[39mtemperature)\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m value[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m value[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteering vec too large\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     60\u001b[0m     value2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m value[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     61\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;28miter\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mabs\u001b[39m(\u001b[38;5;28mint\u001b[39m(output_list_test[sample])\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mint\u001b[39m(value2))\n",
      "\u001b[1;31mAssertionError\u001b[0m: steering vec too large"
     ]
    }
   ],
   "source": [
    "#layer28\n",
    "loss_layer28 = {}\n",
    "for mult in range(1,15):\n",
    "    multiplier = mult\n",
    "    steering_vec = torch.load(f\"./steering_vectors/n_vector28.pt\")\n",
    "    loss = calc_loss_steering_vector(generator, steering_vec, (input_list_test, output_list_test), layer=28, iter=len(input_list_test), multiplier=multiplier)\n",
    "    print(f\"norm {k} loss for layer 28 is {loss}\")\n",
    "    loss_layer28[mult] = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ribbe\\AppData\\Local\\Temp\\ipykernel_472\\2319997551.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  steering_vec = torch.load(f\"./steering_vectors/n_vector16.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm 26 loss for layer 16 is 0.5877796901893274\n",
      "norm 26 loss for layer 16 is 0.5352839931153145\n",
      "norm 26 loss for layer 16 is 0.5051635111876022\n",
      "norm 26 loss for layer 16 is 0.4948364888123869\n",
      "norm 26 loss for layer 16 is 0.5722891566265039\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "steering vec too large",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m multiplier \u001b[38;5;241m=\u001b[39m mult\n\u001b[0;32m      5\u001b[0m steering_vec \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./steering_vectors/n_vector16.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_loss_steering_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteering_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_list_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_list_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_list_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiplier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultiplier\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnorm \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m loss for layer 16 is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m loss_layer16[mult] \u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\eval\\eval.py:59\u001b[0m, in \u001b[0;36mcalc_loss_steering_vector\u001b[1;34m(generator, steering_vec, data, layer, iter, multiplier)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28miter\u001b[39m):\n\u001b[0;32m     55\u001b[0m     value, activations \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39mchat_completion([input_list_test[sample]],\n\u001b[0;32m     56\u001b[0m                                       max_gen_len\u001b[38;5;241m=\u001b[39mmax_gen_len,\n\u001b[0;32m     57\u001b[0m                                        top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m     58\u001b[0m                                        temperature\u001b[38;5;241m=\u001b[39mtemperature)\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m value[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m value[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteering vec too large\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     60\u001b[0m     value2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m value[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     61\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;28miter\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mabs\u001b[39m(\u001b[38;5;28mint\u001b[39m(output_list_test[sample])\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mint\u001b[39m(value2))\n",
      "\u001b[1;31mAssertionError\u001b[0m: steering vec too large"
     ]
    }
   ],
   "source": [
    "#layer16\n",
    "loss_layer16 = {}\n",
    "for mult in range(1,15):\n",
    "    multiplier = mult\n",
    "    steering_vec = torch.load(f\"./steering_vectors/n_vector16.pt\")\n",
    "    loss = calc_loss_steering_vector(generator, steering_vec, (input_list_test, output_list_test), layer=16, iter=len(input_list_test), multiplier=multiplier)\n",
    "    print(f\"norm {k} loss for layer 16 is {loss}\")\n",
    "    loss_layer16[mult] = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ribbe\\AppData\\Local\\Temp\\ipykernel_472\\2298620132.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  steering_vec = torch.load(f\"./steering_vectors/n_vector1.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm 26 loss for layer 28 is 0.6583476764199677\n",
      "norm 26 loss for layer 28 is 0.6075731497418241\n",
      "norm 26 loss for layer 28 is 0.6127366609294319\n",
      "norm 26 loss for layer 28 is 0.6135972461273665\n",
      "norm 26 loss for layer 28 is 0.6282271944922554\n",
      "norm 26 loss for layer 28 is 0.6531841652323599\n",
      "norm 26 loss for layer 28 is 0.6850258175559415\n",
      "norm 26 loss for layer 28 is 0.6712564543889873\n"
     ]
    }
   ],
   "source": [
    "loss_layer1 = {}\n",
    "for mult in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8]:\n",
    "    multiplier = mult\n",
    "    steering_vec = torch.load(f\"./steering_vectors/n_vector1.pt\")\n",
    "    loss = calc_loss_steering_vector(generator, steering_vec, (input_list_test, output_list_test), layer=1, iter=len(input_list_test), multiplier=multiplier)\n",
    "    print(f\"norm {k} loss for layer 28 is {loss}\")\n",
    "    loss_layer1[k] = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ribbe\\AppData\\Local\\Temp\\ipykernel_472\\3694834677.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  stervec = torch.load(f\"./steering_vectors/vector{k}.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4121) 0\n",
      "tensor(1.8594) 1\n",
      "tensor(1.8281) 2\n",
      "tensor(2.1094) 3\n",
      "tensor(2.0625) 4\n",
      "tensor(2.2500) 5\n",
      "tensor(2.3438) 6\n",
      "tensor(2.5781) 7\n",
      "tensor(2.6406) 8\n",
      "tensor(2.7812) 9\n",
      "tensor(2.8281) 10\n",
      "tensor(2.8906) 11\n",
      "tensor(3.3906) 12\n",
      "tensor(3.5938) 13\n",
      "tensor(3.5156) 14\n",
      "tensor(4.) 15\n",
      "tensor(4.1562) 16\n",
      "tensor(4.6250) 17\n",
      "tensor(5.4375) 18\n",
      "tensor(5.6875) 19\n",
      "tensor(6.0938) 20\n",
      "tensor(7.6875) 21\n",
      "tensor(7.8438) 22\n",
      "tensor(10.5625) 23\n",
      "tensor(11.3125) 24\n",
      "tensor(12.1250) 25\n",
      "tensor(12.1250) 26\n",
      "tensor(13.2500) 27\n",
      "tensor(17.8750) 28\n",
      "tensor(20.) 29\n",
      "tensor(22.3750) 30\n",
      "tensor(24.) 31\n"
     ]
    }
   ],
   "source": [
    "for k in range(32):\n",
    "    stervec = torch.load(f\"./steering_vectors/vector{k}.pt\")\n",
    "    print(torch.norm(stervec), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Coding_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
