{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "import fire\n",
    "\n",
    "from llama import Dialog, Llama\n",
    "import torch.distributed as dist\n",
    "import torch\n",
    "import os\n",
    "from data_casual import output_list_train, input_list_train, input_list_test, output_list_test\n",
    "from eval import extracting_steering_vector, calc_loss_steering_vector\n",
    "\n",
    "ckpt_dir = \"./\"\n",
    "tokenizer_path = \"./tokenizer.model\"\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\llama\\generation.py:98: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[43mLlama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(generator)\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\llama\\generation.py:98\u001b[0m, in \u001b[0;36mLlama.build\u001b[1;34m(ckpt_dir, tokenizer_path, max_seq_len, max_batch_size, model_parallel_size, seed, activation, activation_layer, steering_vector)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m model_parallel_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[0;32m     95\u001b[0m     checkpoints\n\u001b[0;32m     96\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading a checkpoint for MP=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(checkpoints)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but world size is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_parallel_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m checkpoints[get_model_parallel_rank()]\n\u001b[1;32m---> 98\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(Path(ckpt_dir) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    100\u001b[0m     params \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(f\u001b[38;5;241m.\u001b[39mread())\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\torch\\serialization.py:1360\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1358\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1359\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1360\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1361\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1362\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1364\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1366\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[0;32m   1368\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\torch\\serialization.py:1848\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[0;32m   1847\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[1;32m-> 1848\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1849\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1851\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\torch\\serialization.py:1812\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1810\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1811\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1812\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1813\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1814\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1816\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\torch\\serialization.py:1772\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   1769\u001b[0m     storage \u001b[38;5;241m=\u001b[39m overall_storage[storage_offset : storage_offset \u001b[38;5;241m+\u001b[39m numel]\n\u001b[0;32m   1770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1771\u001b[0m     storage \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1772\u001b[0m         \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1773\u001b[0m         \u001b[38;5;241m.\u001b[39m_typed_storage()\n\u001b[0;32m   1774\u001b[0m         \u001b[38;5;241m.\u001b[39m_untyped_storage\n\u001b[0;32m   1775\u001b[0m     )\n\u001b[0;32m   1776\u001b[0m \u001b[38;5;66;03m# swap here if byteswapping is needed\u001b[39;00m\n\u001b[0;32m   1777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m byteorderdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generator = Llama.build(ckpt_dir=ckpt_dir, tokenizer_path=tokenizer_path, max_seq_len= 1024, max_batch_size= 4, activation=True, activation_layer=12)\n",
    "print(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.6\n",
    "top_p = 0.9\n",
    "max_seq_len = 1024\n",
    "max_batch_size = 4\n",
    "max_gen_len= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15486585423535665\n"
     ]
    }
   ],
   "source": [
    "#checking the random baseline\n",
    "average_f1 = 0\n",
    "for k in range(1000):\n",
    "    p = output_list_test.count(\"1\")/len(input_list_test)\n",
    "    x = stats.bernoulli.rvs(p, size = len(input_list_test))\n",
    "    difference = [int(x[i] - int(k)) for i,k in enumerate(output_list_test)]\n",
    "    false_pos = difference.count(1)\n",
    "    false_neg = difference.count(-1)\n",
    "    true_pos = [1 for i, k in enumerate(output_list_test) if int(k)==1 and difference[i]==0].count(1)\n",
    "    true_neg = [1 for i, k in enumerate(output_list_test) if int(k)==0 and difference[i]==0].count(1)\n",
    "    precision = true_pos/(true_pos + false_pos)\n",
    "    recall = true_pos/(true_pos + false_neg)\n",
    "    average_f1 += 1/1000*2*(precision*recall)/(precision+recall)\n",
    "print(average_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\eval\\eval.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  instruct_vec = torch.load(f\"./steering_vectors/instruct_vector{layer}.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss is 0.006396609213839021\n",
      "Training loss is 0.006454936957430255\n",
      "Training loss is 0.006377166632641943\n",
      "Training loss is 0.006352863406145596\n",
      "Training loss is 0.0065132647010214895\n",
      "Training loss is 0.006425773085634638\n",
      "Training loss is 0.006459797602729525\n",
      "Training loss is 0.00631397824375144\n",
      "Training loss is 0.006348002760846326\n",
      "Training loss is 0.006425773085634638\n",
      "Training loss is 0.006377166632641943\n",
      "Training loss is 0.006299396307853631\n",
      "Training loss is 0.006377166632641943\n",
      "Training loss is 0.0062653717907587446\n",
      "Training loss is 0.006348002760846326\n",
      "Training loss is 0.006440355021532447\n",
      "Training loss is 0.006382027277941213\n",
      "Training loss is 0.006435494376233177\n",
      "Training loss is 0.006401469859138291\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m average_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m32\u001b[39m):\n\u001b[1;32m----> 4\u001b[0m     steering_vector, loss \u001b[38;5;241m=\u001b[39m \u001b[43mextracting_steering_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_list_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_list_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     steering_vec \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mnormalize(steering_vector, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      6\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(steering_vector, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./vectors_last_token/vector\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\eval\\eval.py:23\u001b[0m, in \u001b[0;36mextracting_steering_vector\u001b[1;34m(generator, data, layer, iter)\u001b[0m\n\u001b[0;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28miter\u001b[39m):\n\u001b[1;32m---> 23\u001b[0m     value, activation_vec_list \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_list_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mmax_gen_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_gen_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     value_numerical \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m value[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value_numerical \u001b[38;5;241m==\u001b[39m output_list_train[i]:\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\llama\\generation.py:325\u001b[0m, in \u001b[0;36mLlama.chat_completion\u001b[1;34m(self, dialogs, temperature, top_p, max_gen_len, logprobs)\u001b[0m\n\u001b[0;32m    320\u001b[0m     max_gen_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mmax_seq_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    322\u001b[0m prompt_tokens \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformatter\u001b[38;5;241m.\u001b[39mencode_dialog_prompt(dialog) \u001b[38;5;28;01mfor\u001b[39;00m dialog \u001b[38;5;129;01min\u001b[39;00m dialogs\n\u001b[0;32m    324\u001b[0m ]\n\u001b[1;32m--> 325\u001b[0m generation_tokens, generation_logprobs, activations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_gen_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_gen_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logprobs:\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    334\u001b[0m         {\n\u001b[0;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m t, logprobs_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(generation_tokens, generation_logprobs)\n\u001b[0;32m    343\u001b[0m     ], activations\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\llama\\generation.py:181\u001b[0m, in \u001b[0;36mLlama.generate\u001b[1;34m(self, prompt_tokens, max_gen_len, temperature, top_p, logprobs, echo)\u001b[0m\n\u001b[0;32m    179\u001b[0m stop_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mstop_tokens))\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cur_pos \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(min_prompt_len, total_len):\n\u001b[1;32m--> 181\u001b[0m     logits, activation_vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_pos\u001b[49m\u001b[43m:\u001b[49m\u001b[43mcur_pos\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_pos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m temperature \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    183\u001b[0m         probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m temperature, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\llama\\model.py:310\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, tokens, start_pos)\u001b[0m\n\u001b[0;32m    308\u001b[0m activation_vector \u001b[38;5;241m=\u001b[39m activation_tensor[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:]\n\u001b[0;32m    309\u001b[0m \u001b[38;5;66;03m#activation_vector = activation_tensor.mean(dim=0)\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mequal(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madding_activation_vector, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcuda()):\n\u001b[0;32m    311\u001b[0m     steering_vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madding_activation_vector\u001b[38;5;241m.\u001b[39mrepeat(h\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    312\u001b[0m     steering_vector \u001b[38;5;241m=\u001b[39m steering_vector\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#extracting a steering vector for every layer\n",
    "average_loss = 0\n",
    "for k in range(6,32):\n",
    "    steering_vector, loss = extracting_steering_vector(generator, (input_list_train, output_list_train),layer=k, iter=2000)\n",
    "    torch.save(steering_vector, f\"./vectors_last_token/full_vector{k}.pt\")\n",
    "    print(f\"Training loss is {loss}\")\n",
    "    average_loss += 1/len(generator.model.layers)*loss\n",
    "print(f\"Average loss is {average_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loss is 0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ribbe\\AppData\\Local\\Temp\\ipykernel_15724\\2526441504.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  steering_vec = torch.load(f\"./vectors_last_token/vector{k}.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 questions were wrongly classified\n",
      "Layer 12 loss with multiplier 1 is 0.6712564543889873\n",
      "7 questions were wrongly classified\n",
      "Layer 12 loss with multiplier 3 is 0.36574870912219953\n",
      "1162 questions were wrongly classified\n",
      "Layer 12 loss with multiplier 5 is 1.0000000000000189\n",
      "0 questions were wrongly classified\n",
      "Layer 13 loss with multiplier 1 is 0.4836488812392373\n",
      "0 questions were wrongly classified\n",
      "Layer 13 loss with multiplier 3 is 0.15490533562822684\n",
      "1162 questions were wrongly classified\n",
      "Layer 13 loss with multiplier 5 is 1.0000000000000189\n",
      "0 questions were wrongly classified\n",
      "Layer 14 loss with multiplier 1 is 0.5154905335628178\n",
      "0 questions were wrongly classified\n",
      "Layer 14 loss with multiplier 3 is 0.2641996557659188\n",
      "1162 questions were wrongly classified\n",
      "Layer 14 loss with multiplier 5 is 1.0000000000000189\n",
      "0 questions were wrongly classified\n",
      "Layer 15 loss with multiplier 1 is 0.7220309810671309\n",
      "0 questions were wrongly classified\n",
      "Layer 15 loss with multiplier 3 is 0.28657487091221795\n",
      "1162 questions were wrongly classified\n",
      "Layer 15 loss with multiplier 5 is 1.0000000000000189\n",
      "0 questions were wrongly classified\n",
      "Layer 16 loss with multiplier 1 is 0.7702237521514707\n",
      "0 questions were wrongly classified\n",
      "Layer 16 loss with multiplier 3 is 0.8442340791738495\n",
      "0 questions were wrongly classified\n",
      "Layer 16 loss with multiplier 5 is 0.8450946643717842\n",
      "1162 questions were wrongly classified\n",
      "Layer 16 loss with multiplier 7 is 1.0000000000000189\n",
      "0 questions were wrongly classified\n",
      "Layer 17 loss with multiplier 1 is 0.6308089500860593\n",
      "0 questions were wrongly classified\n",
      "Layer 17 loss with multiplier 3 is 0.8175559380378757\n",
      "0 questions were wrongly classified\n",
      "Layer 17 loss with multiplier 5 is 0.8450946643717842\n",
      "329 questions were wrongly classified\n",
      "Layer 17 loss with multiplier 7 is 0.8872633390705814\n"
     ]
    }
   ],
   "source": [
    "#calculating the loss for each layes\n",
    "initial_loss, wrong_c = calc_loss_steering_vector(generator, torch.tensor(4096*[0]), (input_list_test, output_list_test), layer=12, iter=len(input_list_test), multiplier=0)\n",
    "f1_scores = {}\n",
    "print(f\"initial f1 is {initial_loss}\")\n",
    "for k in range(6,32):\n",
    "    steering_vec = torch.load(f\"./vectors_last_token/full_vector{k}.pt\")\n",
    "    steering_vec = torch.nn.functional.normalize(steering_vec, dim=0)\n",
    "    for multiplier in range(1,50,2):\n",
    "        f1_score, wrong_classes = calc_loss_steering_vector(generator, steering_vec, (input_list_test, output_list_test), layer=k, iter=len(input_list_test), multiplier=multiplier)\n",
    "        print(f\"Layer {k} loss with multiplier {multiplier} has f1_score {f1_score}\")\n",
    "        f1_scores[f\"{k},{multiplier}\"] = f1_score\n",
    "        if wrong_classes > 50:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loss is 0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ribbe\\AppData\\Local\\Temp\\ipykernel_22676\\1496298509.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  steering_vec = torch.load(f\"./vectors_last_token/vector{k}.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 questions were wrongly classified\n",
      "Layer 18 loss with multiplier 1 is 0.6419965576592096\n",
      "0 questions were wrongly classified\n",
      "Layer 18 loss with multiplier 3 is 0.8055077452667908\n",
      "0 questions were wrongly classified\n",
      "Layer 18 loss with multiplier 5 is 0.8433734939759149\n",
      "0 questions were wrongly classified\n",
      "Layer 18 loss with multiplier 7 is 0.7426850258175622\n",
      "1162 questions were wrongly classified\n",
      "Layer 18 loss with multiplier 9 is 1.0000000000000189\n",
      "0 questions were wrongly classified\n",
      "Layer 19 loss with multiplier 1 is 0.6876075731497454\n",
      "0 questions were wrongly classified\n",
      "Layer 19 loss with multiplier 3 is 0.7960413080895098\n",
      "0 questions were wrongly classified\n",
      "Layer 19 loss with multiplier 5 is 0.8442340791738495\n",
      "0 questions were wrongly classified\n",
      "Layer 19 loss with multiplier 7 is 0.8450946643717842\n",
      "390 questions were wrongly classified\n",
      "Layer 19 loss with multiplier 9 is 0.8958691910499278\n",
      "0 questions were wrongly classified\n",
      "Layer 20 loss with multiplier 1 is 0.7177280550774577\n",
      "0 questions were wrongly classified\n",
      "Layer 20 loss with multiplier 3 is 0.8450946643717842\n",
      "0 questions were wrongly classified\n",
      "Layer 20 loss with multiplier 5 is 0.8450946643717842\n",
      "334 questions were wrongly classified\n",
      "Layer 20 loss with multiplier 7 is 0.8958691910499278\n",
      "0 questions were wrongly classified\n",
      "Layer 21 loss with multiplier 1 is 0.6858864027538761\n",
      "0 questions were wrongly classified\n",
      "Layer 21 loss with multiplier 3 is 0.7659208261617975\n",
      "0 questions were wrongly classified\n",
      "Layer 21 loss with multiplier 5 is 0.8339070567986339\n",
      "650 questions were wrongly classified\n",
      "Layer 21 loss with multiplier 7 is 0.8261617900172221\n",
      "1156 questions were wrongly classified\n",
      "Layer 21 loss with multiplier 9 is 0.9948364888124112\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m steering_vec \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./vectors_last_token/vector\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m multiplier \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m25\u001b[39m,\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m----> 8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_loss_steering_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteering_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_list_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_list_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_list_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiplier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultiplier\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m loss with multiplier \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmultiplier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m     losses_last_token_layer[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmultiplier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\eval\\eval.py:56\u001b[0m, in \u001b[0;36mcalc_loss_steering_vector\u001b[1;34m(generator, steering_vec, data, layer, iter, multiplier)\u001b[0m\n\u001b[0;32m     54\u001b[0m wrong_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28miter\u001b[39m):\n\u001b[1;32m---> 56\u001b[0m     value, activations \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_list_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mmax_gen_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_gen_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     61\u001b[0m         value2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\llama\\generation.py:325\u001b[0m, in \u001b[0;36mLlama.chat_completion\u001b[1;34m(self, dialogs, temperature, top_p, max_gen_len, logprobs)\u001b[0m\n\u001b[0;32m    320\u001b[0m     max_gen_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mmax_seq_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    322\u001b[0m prompt_tokens \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformatter\u001b[38;5;241m.\u001b[39mencode_dialog_prompt(dialog) \u001b[38;5;28;01mfor\u001b[39;00m dialog \u001b[38;5;129;01min\u001b[39;00m dialogs\n\u001b[0;32m    324\u001b[0m ]\n\u001b[1;32m--> 325\u001b[0m generation_tokens, generation_logprobs, activations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_gen_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_gen_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logprobs:\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    334\u001b[0m         {\n\u001b[0;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m t, logprobs_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(generation_tokens, generation_logprobs)\n\u001b[0;32m    343\u001b[0m     ], activations\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\llama\\generation.py:181\u001b[0m, in \u001b[0;36mLlama.generate\u001b[1;34m(self, prompt_tokens, max_gen_len, temperature, top_p, logprobs, echo)\u001b[0m\n\u001b[0;32m    179\u001b[0m stop_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mstop_tokens))\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cur_pos \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(min_prompt_len, total_len):\n\u001b[1;32m--> 181\u001b[0m     logits, activation_vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_pos\u001b[49m\u001b[43m:\u001b[49m\u001b[43mcur_pos\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_pos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m temperature \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    183\u001b[0m         probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m temperature, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\llama\\model.py:304\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, tokens, start_pos)\u001b[0m\n\u001b[0;32m    299\u001b[0m     mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhstack(\n\u001b[0;32m    300\u001b[0m         [torch\u001b[38;5;241m.\u001b[39mzeros((seqlen, start_pos), device\u001b[38;5;241m=\u001b[39mtokens\u001b[38;5;241m.\u001b[39mdevice), mask]\n\u001b[0;32m    301\u001b[0m     )\u001b[38;5;241m.\u001b[39mtype_as(h)\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[1;32m--> 304\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_layer_n \u001b[38;5;241m==\u001b[39m k:\n\u001b[0;32m    306\u001b[0m         activation_tensor \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mclone()\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\llama\\model.py:246\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x, start_pos, freqs_cis, mask)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    241\u001b[0m     x: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    244\u001b[0m     mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[0;32m    245\u001b[0m ):\n\u001b[1;32m--> 246\u001b[0m     h \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, start_pos, freqs_cis, mask)\n\u001b[0;32m    247\u001b[0m     out \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn_norm(h))\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\Coding_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Ribbe\\Coding\\VSC\\FU_Berlin\\Actual_Work\\llama3\\llama\\model.py:45\u001b[0m, in \u001b[0;36mRMSNorm.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 45\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_norm(\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mtype_as(x)\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#calculating the loss for each layes\n",
    "initial_loss, wrong_c = calc_loss_steering_vector(generator, torch.tensor(4096*[0]), (input_list_test, output_list_test), layer=12, iter=len(input_list_test), multiplier=0)\n",
    "f1_scores_centralized = {}\n",
    "print(f\"initial f1 is {initial_loss}\")\n",
    "for k in range(6,32):\n",
    "    steering_vec = torch.load(f\"./vectors_last_token/full_vector{k}.pt\")\n",
    "    steering_vec = steering_vec - torch.mean(steering_vec, dim=0)\n",
    "    steering_vec = torch.nn.functional.normalize(steering_vec, dim=0)\n",
    "    for multiplier in range(1,50,2):\n",
    "        f1_score, wrong_classes = calc_loss_steering_vector(generator, steering_vec, (input_list_test, output_list_test), layer=k, iter=len(input_list_test), multiplier=multiplier)\n",
    "        print(f\"Layer {k} loss with multiplier {multiplier} has f1_score {f1_score}\")\n",
    "        f1_scores_centralized[f\"{k},{multiplier}\"] = f1_score\n",
    "        if wrong_classes > 50:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loss is 0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ribbe\\AppData\\Local\\Temp\\ipykernel_3580\\2461276629.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  steering_vec = torch.load(f\"./c_and_n_vectors/vector{k}.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 16 loss with multiplier 1 is 0.660068846815837\n",
      "Layer 16 loss with multiplier 3 is 0.5955249569707391\n",
      "Layer 16 loss with multiplier 5 is 0.5481927710843341\n",
      "Layer 16 loss with multiplier 7 is 0.44836488812391945\n",
      "Layer 16 loss with multiplier 9 is 0.28227194492254504\n",
      "Layer 16 loss with multiplier 11 is 0.9991394148020843\n",
      "Layer 17 loss with multiplier 1 is 0.7125645438898499\n",
      "Layer 17 loss with multiplier 3 is 0.809810671256464\n",
      "Layer 17 loss with multiplier 5 is 0.8390705679862417\n",
      "Layer 17 loss with multiplier 7 is 0.8450946643717842\n",
      "Layer 17 loss with multiplier 9 is 0.8433734939759149\n",
      "Layer 17 loss with multiplier 11 is 0.8950086058519932\n",
      "Layer 17 loss with multiplier 13 is 0.9991394148020843\n",
      "Layer 18 loss with multiplier 1 is 0.7512908777969086\n",
      "Layer 18 loss with multiplier 3 is 0.8261617900172221\n",
      "Layer 18 loss with multiplier 5 is 0.838209982788307\n",
      "Layer 18 loss with multiplier 7 is 0.8347676419965685\n",
      "Layer 18 loss with multiplier 9 is 0.8046471600688562\n",
      "Layer 18 loss with multiplier 11 is 0.6841652323580069\n",
      "Layer 18 loss with multiplier 13 is 0.9931153184165419\n",
      "Layer 19 loss with multiplier 1 is 0.6592082616179024\n",
      "Layer 19 loss with multiplier 3 is 0.700516351118765\n",
      "Layer 19 loss with multiplier 5 is 0.7607573149741896\n",
      "Layer 19 loss with multiplier 7 is 0.709982788296046\n",
      "Layer 19 loss with multiplier 9 is 0.4991394148020598\n",
      "Layer 19 loss with multiplier 11 is 0.3141135972461246\n",
      "Layer 19 loss with multiplier 13 is 0.9819277108433916\n",
      "Layer 20 loss with multiplier 1 is 0.6230636833046476\n",
      "Layer 20 loss with multiplier 3 is 0.6901893287435493\n",
      "Layer 20 loss with multiplier 5 is 0.7340791738382159\n",
      "Layer 20 loss with multiplier 7 is 0.679001721170399\n",
      "Layer 20 loss with multiplier 9 is 0.46299483648880735\n",
      "Layer 20 loss with multiplier 11 is 0.26678141135972255\n",
      "Layer 20 loss with multiplier 13 is 0.8580034423408037\n",
      "Layer 20 loss with multiplier 15 is 1.0000000000000189\n",
      "Layer 21 loss with multiplier 1 is 0.6531841652323599\n",
      "Layer 21 loss with multiplier 3 is 0.6807228915662683\n",
      "Layer 21 loss with multiplier 5 is 0.6333907056798632\n",
      "Layer 21 loss with multiplier 7 is 0.5111876075731446\n",
      "Layer 21 loss with multiplier 9 is 0.2848537005163488\n",
      "Layer 21 loss with multiplier 11 is 0.21686746987951677\n",
      "Layer 21 loss with multiplier 13 is 0.6643717728055102\n",
      "Layer 21 loss with multiplier 15 is 0.9870912220309994\n",
      "Layer 22 loss with multiplier 1 is 0.7030981067125689\n",
      "Layer 22 loss with multiplier 3 is 0.7762478485370131\n",
      "Layer 22 loss with multiplier 5 is 0.809810671256464\n",
      "Layer 22 loss with multiplier 7 is 0.8012048192771176\n",
      "Layer 22 loss with multiplier 9 is 0.7263339070568041\n",
      "Layer 22 loss with multiplier 11 is 0.6316695352839939\n",
      "Layer 22 loss with multiplier 13 is 0.8614457831325423\n",
      "Layer 22 loss with multiplier 15 is 0.9965576592082804\n",
      "Layer 23 loss with multiplier 1 is 0.7203098106712617\n",
      "Layer 23 loss with multiplier 3 is 0.819277108433745\n",
      "Layer 23 loss with multiplier 5 is 0.8227194492254836\n",
      "Layer 23 loss with multiplier 7 is 0.7762478485370131\n",
      "Layer 23 loss with multiplier 9 is 0.6583476764199677\n",
      "Layer 23 loss with multiplier 11 is 0.4302925989672932\n",
      "Layer 23 loss with multiplier 13 is 0.5516351118760726\n",
      "Layer 23 loss with multiplier 15 is 0.9440619621342675\n",
      "Layer 24 loss with multiplier 1 is 0.6402753872633403\n",
      "Layer 24 loss with multiplier 3 is 0.5903614457831313\n",
      "Layer 24 loss with multiplier 5 is 0.49741824440619065\n",
      "Layer 24 loss with multiplier 7 is 0.3390705679862275\n",
      "Layer 24 loss with multiplier 9 is 0.2254733218588626\n",
      "Layer 24 loss with multiplier 11 is 0.19449225473321763\n",
      "Layer 24 loss with multiplier 13 is 0.5464716006884648\n",
      "Layer 24 loss with multiplier 15 is 0.9475043029260061\n",
      "Layer 25 loss with multiplier 1 is 0.6540447504302945\n",
      "Layer 25 loss with multiplier 3 is 0.6024096385542163\n",
      "Layer 25 loss with multiplier 5 is 0.4888123924268448\n",
      "Layer 25 loss with multiplier 7 is 0.41308089500860157\n",
      "Layer 25 loss with multiplier 9 is 0.32013769363166666\n",
      "Layer 25 loss with multiplier 11 is 0.5223752151462949\n",
      "Layer 25 loss with multiplier 13 is 0.9087779690189474\n",
      "Layer 26 loss with multiplier 1 is 0.6394148020654057\n",
      "Layer 26 loss with multiplier 3 is 0.5989672977624777\n",
      "Layer 26 loss with multiplier 5 is 0.5826161790017196\n",
      "Layer 26 loss with multiplier 7 is 0.5215146299483603\n",
      "Layer 26 loss with multiplier 9 is 0.41824440619620906\n",
      "Layer 26 loss with multiplier 11 is 0.32099827882960125\n",
      "Layer 26 loss with multiplier 13 is 0.23321858864027384\n",
      "Layer 26 loss with multiplier 15 is 0.2099827882960401\n",
      "Layer 26 loss with multiplier 17 is 0.2977624784853675\n",
      "Layer 26 loss with multiplier 19 is 0.5464716006884648\n",
      "Layer 26 loss with multiplier 21 is 0.8175559380378757\n",
      "Layer 26 loss with multiplier 23 is 0.9716006884681759\n",
      "Layer 27 loss with multiplier 1 is 0.6376936316695364\n",
      "Layer 27 loss with multiplier 3 is 0.5808950086058503\n",
      "Layer 27 loss with multiplier 5 is 0.5309810671256413\n",
      "Layer 27 loss with multiplier 7 is 0.4716006884681532\n",
      "Layer 27 loss with multiplier 9 is 0.4113597246127324\n",
      "Layer 27 loss with multiplier 11 is 0.323580034423405\n",
      "Layer 27 loss with multiplier 13 is 0.31583476764199375\n",
      "Layer 27 loss with multiplier 15 is 0.5456110154905301\n",
      "Layer 27 loss with multiplier 17 is 0.8373493975903724\n",
      "Layer 27 loss with multiplier 19 is 0.9750430292599145\n",
      "Layer 28 loss with multiplier 1 is 0.6316695352839939\n",
      "Layer 28 loss with multiplier 3 is 0.5731497418244386\n",
      "Layer 28 loss with multiplier 5 is 0.5215146299483603\n",
      "Layer 28 loss with multiplier 7 is 0.43889845094663904\n",
      "Layer 28 loss with multiplier 9 is 0.39500860585197534\n",
      "Layer 28 loss with multiplier 11 is 0.30636833046471335\n",
      "Layer 28 loss with multiplier 13 is 0.25817555938037673\n",
      "Layer 28 loss with multiplier 15 is 0.21772805507745135\n",
      "Layer 28 loss with multiplier 17 is 0.25731497418244215\n",
      "Layer 28 loss with multiplier 19 is 0.4259896729776203\n",
      "Layer 28 loss with multiplier 21 is 0.6876075731497454\n",
      "Layer 28 loss with multiplier 23 is 0.8872633390705814\n",
      "Layer 28 loss with multiplier 25 is 0.9827882960413262\n",
      "Layer 29 loss with multiplier 1 is 0.6299483648881247\n",
      "Layer 29 loss with multiplier 3 is 0.6041308089500855\n",
      "Layer 29 loss with multiplier 5 is 0.5757314974182425\n",
      "Layer 29 loss with multiplier 7 is 0.5567986230636804\n",
      "Layer 29 loss with multiplier 9 is 0.5395869191049877\n",
      "Layer 29 loss with multiplier 11 is 0.5068846815834714\n",
      "Layer 29 loss with multiplier 13 is 0.4664371772805457\n",
      "Layer 29 loss with multiplier 15 is 0.4552495697073961\n",
      "Layer 29 loss with multiplier 17 is 0.37091222030980703\n",
      "Layer 29 loss with multiplier 19 is 0.35025817555937705\n",
      "Layer 29 loss with multiplier 21 is 0.3175559380378629\n",
      "Layer 29 loss with multiplier 23 is 0.3769363166953491\n",
      "Layer 29 loss with multiplier 25 is 0.5301204819277067\n",
      "Layer 29 loss with multiplier 27 is 0.691049913941484\n",
      "Layer 29 loss with multiplier 29 is 0.8227194492254836\n",
      "Layer 29 loss with multiplier 31 is 0.9079173838210127\n",
      "Layer 30 loss with multiplier 1 is 0.641135972461275\n",
      "Layer 30 loss with multiplier 3 is 0.6480206540447521\n",
      "Layer 30 loss with multiplier 5 is 0.6256454388984515\n",
      "Layer 30 loss with multiplier 7 is 0.6247848537005168\n",
      "Layer 30 loss with multiplier 9 is 0.6049913941480202\n",
      "Layer 30 loss with multiplier 11 is 0.5920826161790006\n",
      "Layer 30 loss with multiplier 13 is 0.5619621342512883\n",
      "Layer 30 loss with multiplier 15 is 0.5524956970740073\n",
      "Layer 30 loss with multiplier 17 is 0.5619621342512883\n",
      "Layer 30 loss with multiplier 19 is 0.5223752151462949\n",
      "Layer 30 loss with multiplier 21 is 0.5266781411359681\n",
      "Layer 30 loss with multiplier 23 is 0.49311531841651773\n",
      "Layer 30 loss with multiplier 25 is 0.45352839931152694\n",
      "Layer 30 loss with multiplier 27 is 0.4543889845094615\n",
      "Layer 30 loss with multiplier 29 is 0.42340791738381656\n",
      "Layer 30 loss with multiplier 31 is 0.3872633390705641\n",
      "Layer 30 loss with multiplier 33 is 0.38554216867469493\n",
      "Layer 30 loss with multiplier 35 is 0.36919104991393786\n",
      "Layer 30 loss with multiplier 37 is 0.38037865748708743\n",
      "Layer 30 loss with multiplier 39 is 0.4354561101549007\n",
      "Layer 31 loss with multiplier 1 is 0.6342512908777979\n",
      "Layer 31 loss with multiplier 3 is 0.619621342512909\n",
      "Layer 31 loss with multiplier 5 is 0.6161790017211705\n",
      "Layer 31 loss with multiplier 7 is 0.6084337349397587\n",
      "Layer 31 loss with multiplier 9 is 0.6024096385542163\n",
      "Layer 31 loss with multiplier 11 is 0.5654044750430268\n",
      "Layer 31 loss with multiplier 13 is 0.5490533562822687\n",
      "Layer 31 loss with multiplier 15 is 0.5206540447504256\n",
      "Layer 31 loss with multiplier 17 is 0.5172117039586871\n",
      "Layer 31 loss with multiplier 19 is 0.5043029259896675\n",
      "Layer 31 loss with multiplier 21 is 0.469879518072284\n",
      "Layer 31 loss with multiplier 23 is 0.43803786574870446\n",
      "Layer 31 loss with multiplier 25 is 0.43459552495696613\n",
      "Layer 31 loss with multiplier 27 is 0.42943201376935863\n",
      "Layer 31 loss with multiplier 29 is 0.4173838209982745\n",
      "Layer 31 loss with multiplier 31 is 0.4113597246127324\n",
      "Layer 31 loss with multiplier 33 is 0.4061962134251249\n",
      "Layer 31 loss with multiplier 35 is 0.38984509466436784\n",
      "Layer 31 loss with multiplier 37 is 0.34079173838209664\n",
      "Layer 31 loss with multiplier 39 is 0.3571428571428537\n"
     ]
    }
   ],
   "source": [
    "#calculating the loss for each layes\n",
    "#initial_loss = calc_loss_steering_vector(generator, torch.tensor(4096*[0]), (input_list_test, output_list_test), layer=12, iter=len(input_list_test), multiplier=0)\n",
    "print(f\"initial loss is {initial_loss}\")\n",
    "for k in range(16,32):\n",
    "    steering_vec = torch.load(f\"./c_and_n_vectors/vector{k}.pt\")\n",
    "    for multiplier in range(1,40,2):\n",
    "        loss = calc_loss_steering_vector(generator, steering_vec, (input_list_test, output_list_test), layer=k, iter=len(input_list_test), multiplier=multiplier)\n",
    "        print(f\"Layer {k} loss with multiplier {multiplier} is {loss}\")\n",
    "        losses2[f\"{k},{multiplier}\"] = loss\n",
    "        if loss>0.9:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Coding_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
