{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer where it works well : without instruct vector layer 24, multiplier 11 or 26, 15 or 16,9\n",
    "#layers where it works badly: without instruct vector layer22, 17, 23\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as nn\n",
    "from typing import List, Optional\n",
    "\n",
    "import fire\n",
    "\n",
    "from llama import Dialog, Llama\n",
    "import torch.distributed as dist\n",
    "import os\n",
    "from data_casual import output_list_train, input_list_train, input_list_test, output_list_test\n",
    "from eval import extracting_steering_vector, calc_loss_steering_vector\n",
    "\n",
    "ckpt_dir = \"./\"\n",
    "tokenizer_path = \"./tokenizer.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Llama.build(\"./\", \"./tokenizer.model\",max_seq_len= 2500,max_batch_size= 4, activation=True, activation_layer=24, steering_vector=torch.tensor(4096*[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want to see which of our proposed vectors work the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_loss = 0.68\n",
    "losses_normal_vectors = {}\n",
    "print(f\"initial loss is {initial_loss}\")\n",
    "for k in range(12,29):\n",
    "    steering_vec = torch.load(f\"./steering_vectors/n_vector{k}.pt\")\n",
    "    for multiplier in range(1,20,2):\n",
    "        loss = calc_loss_steering_vector(generator, steering_vec, (input_list_test, output_list_test), layer=k, iter=len(input_list_test), multiplier=multiplier)\n",
    "        print(f\"Layer {k} loss with multiplier {multiplier} is {loss}\")\n",
    "        losses_normal_vectors[f\"{k},{multiplier}\"] = loss\n",
    "        if loss>0.8:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_loss = 0.68\n",
    "losses_wo_instruct_vectors = {}\n",
    "print(f\"initial loss is {initial_loss}\")\n",
    "for k in range(12,29):\n",
    "    steering_vec = torch.load(f\"./steering_without_instruct/n_vector{k}.pt\")\n",
    "    for multiplier in range(1,40,2):\n",
    "        loss = calc_loss_steering_vector(generator, steering_vec, (input_list_test, output_list_test), layer=k, iter=len(input_list_test), multiplier=multiplier)\n",
    "        print(f\"Layer {k} loss with multiplier {multiplier} is {loss}\")\n",
    "        losses_wo_instruct_vectors[f\"{k},{multiplier}\"] = loss\n",
    "        if loss>0.8:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_loss = 0.68\n",
    "losses_centralized_vectors = {}\n",
    "print(f\"initial loss is {initial_loss}\")\n",
    "for k in range(12,29):\n",
    "    steering_vec = torch.load(f\"./c_and_n_vectors/vector{k}.pt\")\n",
    "    for multiplier in range(1,40,2):\n",
    "        loss = calc_loss_steering_vector(generator, steering_vec, (input_list_test, output_list_test), layer=k, iter=len(input_list_test), multiplier=multiplier)\n",
    "        print(f\"Layer {k} loss with multiplier {multiplier} is {loss}\")\n",
    "        losses_centralized_vectors[f\"{k},{multiplier}\"] = loss\n",
    "        if loss>0.8:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Coding_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
